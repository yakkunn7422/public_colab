{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakkunn7422/public_colab/blob/main/ai_contest_thema2_r8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yV094ubTpxU",
        "outputId": "4585f576-0f3d-4cac-c711-c0b889790d16"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Signateから 学習・評価用のデータ一式をダウンロード\n",
        "\n",
        "# 1. SIGNATE(https://signate.jp/) でアカウント登録\n",
        "# 初めてアクセスする際は、「コンペティションへの同意」を聞かれるので y 入力 必要\n",
        "\n",
        "!pip install signate\n",
        "!signate token --email=yakkunn7422@gmail.com --password=signate\n",
        "#!signate list\n",
        "!signate files --competition-id=270\n",
        "\n",
        "# 応募用\n",
        "!signate download --competition-id=270 --file-id=758 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=759 --path=/content/sample_data/\n",
        "\n",
        "# 衛星画像\n",
        "!signate download --competition-id=270 --file-id=755 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=756 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=757 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=762 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=763 --path=/content/sample_data/\n",
        "\n",
        "#気象情報\n",
        "\n",
        "!signate download --competition-id=270 --file-id=751 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=752 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=753 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=754 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=764 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=765 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=789 --path=/content/sample_data/\n",
        "\n"
      ],
      "metadata": {
        "id": "fLvnZzsiUt99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56309c0e-d65c-4a95-ebe3-e976c6c678de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting signate\n",
            "  Downloading signate-0.9.10-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from signate) (8.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from signate) (0.9.0)\n",
            "Collecting wget (from signate)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from signate) (2.4.0)\n",
            "Requirement already satisfied: six>=1.16 in /usr/local/lib/python3.11/dist-packages (from signate) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from signate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from signate) (2.9.0.post0)\n",
            "Downloading signate-0.9.10-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=d1445ebbd6d87f7a893a43b116c325c502b481594cbcc3f776cd3f572424884f\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, signate\n",
            "Successfully installed signate-0.9.10 wget-3.2\n",
            "\u001b[32mThe API Token has been downloaded successfully.\u001b[0m\n",
            "  fileId  name                   title                         size  updated_at\n",
            "--------  ---------------------  ----------------------  ----------  -------------------\n",
            "     757  sat_image_2018.zip     衛星画像2018年          1196800853  2019-09-10 02:36:11\n",
            "     755  sat_image_2017_01.zip  衛星画像2017年前半      1100993010  2019-09-10 02:36:11\n",
            "     756  sat_image_2017_02.zip  衛星画像2017年後半      1109747198  2019-09-10 02:36:11\n",
            "     762  sat_image_2016_01.zip  衛星画像2016年前半      1111602835  2019-09-10 02:36:11\n",
            "     763  sat_image_2016_02.zip  衛星画像2016年後半      1119587373  2019-09-10 02:36:11\n",
            "     753  met_data_2018_01.zip   気象データ2018年前半    1109284006  2019-09-10 02:36:11\n",
            "     754  met_data_2018_02.zip   気象データ2018年後半    1198905910  2019-09-10 02:36:11\n",
            "     751  met_data_2017_01.zip   気象データ2017年前半    2028546707  2019-09-10 02:36:11\n",
            "     752  met_data_2017_02.zip   気象データ2017年後半    2052181433  2019-09-10 02:36:11\n",
            "     764  met_data_2016_01.zip   気象データ2016年前半    2037719833  2019-09-10 02:36:11\n",
            "     765  met_data_2016_02.zip   気象データ2016年後半    2058580509  2019-09-10 02:36:11\n",
            "     789  add_met_data_2016.zip  気象データ2016年追加        120157  2019-09-10 02:36:11\n",
            "     758  inference_terms.csv    評価期間の詳細日時情報        6425  2019-09-10 02:20:11\n",
            "     759  sample_submit.csv      応募用サンプルファイル   102412552  2019-09-10 02:20:11\n",
            "inference_terms.csv\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sample_submit.csv\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2017_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2017_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2018.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2016_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2016_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2017_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2017_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2018_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2018_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2016_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2016_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "add_met_data_2016.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nrYQAWjCnzEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zipファイルをすべて解凍する。\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "source_folder      = '/content/sample_data/'  # zipファイルのおいている場所\n",
        "destination_folder = '/content/sample_data/'  # 解凍先\n",
        "\n",
        "# 衛星画像、気象データ対象ファイルのリスト（ファイル名だけ）\n",
        "zip_files = ['sat_image_2017_01.zip', 'sat_image_2017_02.zip', 'sat_image_2016_01.zip','sat_image_2016_02.zip',\n",
        "             'met_data_2017_01.zip','met_data_2017_02.zip','met_data_2016_01.zip','met_data_2016_02.zip','add_met_data_2016.zip',\n",
        "             'sat_image_2018.zip','met_data_2018_01.zip','met_data_2018_02.zip'\n",
        "             ]\n",
        "\n",
        "# zipファイルを解凍する関数\n",
        "def unzip_files(file_list, source_dir, extract_to):\n",
        "    os.makedirs(extract_to, exist_ok=True)  # 解凍先フォルダがなければ作る\n",
        "\n",
        "    for file_name in file_list:\n",
        "        zip_path = os.path.join(source_dir, file_name)  # YYYとファイル名を結合\n",
        "        if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "                print(f\"解凍しました: {zip_path} → {extract_to}\")\n",
        "        else:\n",
        "            print(f\"ファイルが存在しません。スキップします。: {zip_path}\")\n",
        "\n",
        "# 衛星画像、気象データ対象ファイル を解凍\n",
        "unzip_files(zip_files, source_folder, destination_folder)\n",
        "\n",
        "# 追加のデータを所定のフォルダにCOPY\n",
        "path    = os.path.join(destination_folder, 'data')\n",
        "if os.path.isdir(path):\n",
        "    shutil.copy2(\"/content/sample_data/data/HGT.300.3.2016010718.gz\", \"/content/sample_data/train/met/2016/01/07/\")\n",
        "    shutil.copy2(\"/content/sample_data/data/RH.1p5m.3.2016011218.gz\", \"/content/sample_data/train/met/2016/01/12/\")\n",
        "    shutil.copy2(\"/content/sample_data/data/TMP.850.3.2016011209.gz\", \"/content/sample_data/train/met/2016/01/12/\")\n",
        "    print('ファイルCOPY END')"
      ],
      "metadata": {
        "id": "oCOzT71KHNHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3afe3d-b7d9-454d-8c04-47b053be7eb7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "解凍しました: /content/sample_data/sat_image_2017_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2017_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2016_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2016_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2017_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2017_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2016_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2016_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/add_met_data_2016.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2018.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2018_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2018_02.zip → /content/sample_data/\n",
            "ファイルCOPY END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lrta /content/sample_data/train/met/2016/01/01"
      ],
      "metadata": {
        "id": "qY3cFCl8EMhV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bebdd6a-e980-4243-ac2a-50d86793f5e0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 11464\n",
            "-rw-r--r--  1 root root 32482 May  3 04:46 HGT.500.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 33955 May  3 04:46 HGT.500.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 33312 May  3 04:46 VVEL.700.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 45865 May  3 04:46 RH.850.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 54147 May  3 04:46 UGRD.10m.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 41472 May  3 04:46 RH.700.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 40935 May  3 04:46 UGRD.200.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 47916 May  3 04:46 PRMSL.msl.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 42558 May  3 04:46 RH.700.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 35788 May  3 04:46 HGT.850.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 43211 May  3 04:46 VGRD.500.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 37524 May  3 04:46 TMP.200.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 44556 May  3 04:46 VGRD.200.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 44280 May  3 04:46 TMP.1p5m.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 54669 May  3 04:46 VGRD.10m.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 34365 May  3 04:46 TMP.500.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 34482 May  3 04:46 TMP.500.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 38157 May  3 04:46 VVEL.850.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 40599 May  3 04:46 VVEL.200.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 43574 May  3 04:46 VGRD.300.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 40595 May  3 04:46 UGRD.500.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 39791 May  3 04:46 UGRD.700.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 38129 May  3 04:46 TMP.700.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 43259 May  3 04:46 TMP.1p5m.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 54037 May  3 04:46 UGRD.10m.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 37006 May  3 04:46 TMP.300.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 41812 May  3 04:46 VGRD.500.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 45411 May  3 04:46 RH.850.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 40042 May  3 04:46 UGRD.300.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 43938 May  3 04:46 RH.300.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 35708 May  3 04:46 VVEL.300.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 44915 May  3 04:46 VGRD.700.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 35766 May  3 04:46 VVEL.300.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 45782 May  3 04:46 VGRD.700.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 41309 May  3 04:46 UGRD.300.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 43720 May  3 04:46 VGRD.500.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 35291 May  3 04:46 TMP.300.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 39210 May  3 04:46 UGRD.700.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 41158 May  3 04:46 RH.700.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 39265 May  3 04:46 UGRD.500.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 35661 May  3 04:46 TMP.700.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 32809 May  3 04:46 VVEL.700.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 42260 May  3 04:46 VGRD.300.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 37723 May  3 04:46 VVEL.850.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 36573 May  3 04:46 VVEL.700.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 54847 May  3 04:46 VGRD.10m.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 34658 May  3 04:46 TMP.500.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 35953 May  3 04:46 HGT.500.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 38757 May  3 04:46 VVEL.300.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 39852 May  3 04:46 UGRD.500.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 42200 May  3 04:46 VGRD.200.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 54923 May  3 04:46 VGRD.10m.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 37839 May  3 04:46 VVEL.850.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 41636 May  3 04:46 RH.300.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 43793 May  3 04:46 TMP.1p5m.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 41363 May  3 04:46 RH.300.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 40386 May  3 04:46 UGRD.200.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 38818 May  3 04:46 TMP.200.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 32558 May  3 04:46 HGT.850.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 54437 May  3 04:46 UGRD.10m.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 48980 May  3 04:46 PRMSL.msl.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 36922 May  3 04:46 VVEL.200.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 45433 May  3 04:46 RH.850.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 37147 May  3 04:46 VVEL.200.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 54114 May  3 04:46 UGRD.10m.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 42514 May  3 04:46 UGRD.850.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 48666 May  3 04:46 PRMSL.msl.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 40941 May  3 04:46 UGRD.200.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 45185 May  3 04:46 RH.850.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 34142 May  3 04:46 HGT.700.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 37658 May  3 04:46 VVEL.500.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41494 May  3 04:46 UGRD.300.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 32187 May  3 04:46 HGT.850.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 35112 May  3 04:46 HGT.700.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 45225 May  3 04:46 VGRD.700.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 38458 May  3 04:46 TMP.200.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 41786 May  3 04:46 RH.500.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 41217 May  3 04:46 VGRD.200.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 42569 May  3 04:46 RH.500.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 54067 May  3 04:46 VGRD.10m.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 35845 May  3 04:46 TMP.700.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 35221 May  3 04:46 TMP.700.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 44657 May  3 04:46 TMP.1p5m.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 40464 May  3 04:46 UGRD.700.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 36188 May  3 04:46 TMP.850.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 32882 May  3 04:46 HGT.200.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 36025 May  3 04:46 TMP.850.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 32299 May  3 04:46 HGT.200.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 37759 May  3 04:46 VVEL.850.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 42618 May  3 04:46 VGRD.200.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 33932 May  3 04:46 TMP.500.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 44207 May  3 04:46 VGRD.300.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 39635 May  3 04:46 UGRD.500.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 46037 May  3 04:46 VGRD.850.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 46514 May  3 04:46 VGRD.850.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 45124 May  3 04:46 VGRD.700.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 37755 May  3 04:46 TMP.300.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 34767 May  3 04:46 HGT.300.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 49498 May  3 04:46 RH.1p5m.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 49915 May  3 04:46 PRMSL.msl.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 38918 May  3 04:46 TMP.200.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 37237 May  3 04:46 HGT.850.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 45390 May  3 04:46 VGRD.500.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 41014 May  3 04:46 UGRD.300.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 42469 May  3 04:46 VGRD.500.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 41994 May  3 04:46 UGRD.300.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 46504 May  3 04:46 VGRD.700.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 41160 May  3 04:46 UGRD.200.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 36174 May  3 04:46 TMP.300.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 39687 May  3 04:46 UGRD.500.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 41389 May  3 04:46 RH.500.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 43350 May  3 04:46 VGRD.300.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 34476 May  3 04:46 TMP.500.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 37653 May  3 04:46 VVEL.850.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 37018 May  3 04:46 VVEL.500.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 41787 May  3 04:46 UGRD.850.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 39334 May  3 04:46 UGRD.700.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 36116 May  3 04:46 HGT.700.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41090 May  3 04:46 UGRD.850.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 35920 May  3 04:46 TMP.700.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 33884 May  3 04:46 VVEL.500.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 43971 May  3 04:46 TMP.1p5m.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 40308 May  3 04:46 UGRD.700.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 49515 May  3 04:46 RH.1p5m.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 32794 May  3 04:46 HGT.300.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 49820 May  3 04:46 RH.1p5m.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 31824 May  3 04:46 HGT.300.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 43759 May  3 04:46 VGRD.300.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 54906 May  3 04:46 VGRD.10m.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 42803 May  3 04:46 VGRD.850.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 42723 May  3 04:46 VGRD.200.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 36594 May  3 04:46 TMP.850.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 38924 May  3 04:46 TMP.200.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 34413 May  3 04:46 HGT.200.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 37390 May  3 04:46 TMP.300.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 33875 May  3 04:46 HGT.850.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 40724 May  3 04:46 UGRD.200.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 54268 May  3 04:46 UGRD.10m.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 45490 May  3 04:46 RH.850.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 49608 May  3 04:46 RH.1p5m.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 50779 May  3 04:46 PRMSL.msl.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 33734 May  3 04:46 VVEL.500.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 41712 May  3 04:46 RH.300.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 42237 May  3 04:46 UGRD.300.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 33260 May  3 04:46 HGT.300.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 41481 May  3 04:46 UGRD.850.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 36990 May  3 04:46 VVEL.200.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 42794 May  3 04:46 VGRD.700.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41260 May  3 04:46 RH.500.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 40497 May  3 04:46 UGRD.200.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 34828 May  3 04:46 HGT.500.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 39852 May  3 04:46 UGRD.200.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 36246 May  3 04:46 TMP.700.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41744 May  3 04:46 RH.700.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 35546 May  3 04:46 TMP.300.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 36547 May  3 04:46 HGT.700.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 38345 May  3 04:46 VVEL.300.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 35276 May  3 04:46 TMP.300.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 42873 May  3 04:46 VGRD.850.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 41942 May  3 04:46 VGRD.300.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 43190 May  3 04:46 VGRD.200.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 36907 May  3 04:46 VVEL.700.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 43196 May  3 04:46 VGRD.300.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 37414 May  3 04:46 VVEL.500.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 39503 May  3 04:46 UGRD.700.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 49444 May  3 04:46 RH.1p5m.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 33694 May  3 04:46 HGT.850.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 34094 May  3 04:46 HGT.200.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 36341 May  3 04:46 TMP.850.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 38816 May  3 04:46 TMP.200.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41889 May  3 04:46 UGRD.850.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 51038 May  3 04:46 PRMSL.msl.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 42527 May  3 04:46 UGRD.700.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 33879 May  3 04:46 HGT.300.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 42009 May  3 04:46 UGRD.850.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 38609 May  3 04:46 UGRD.200.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 37994 May  3 04:46 TMP.700.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 36340 May  3 04:46 TMP.850.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 38352 May  3 04:46 TMP.700.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 36762 May  3 04:46 VVEL.700.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 36951 May  3 04:46 VVEL.500.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 33235 May  3 04:46 HGT.200.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 45914 May  3 04:46 VGRD.850.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 45388 May  3 04:46 VGRD.700.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 35771 May  3 04:46 VVEL.300.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 44124 May  3 04:46 VGRD.700.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 40762 May  3 04:46 UGRD.300.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 41106 May  3 04:46 RH.500.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 40239 May  3 04:46 UGRD.300.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 32030 May  3 04:46 HGT.850.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 35825 May  3 04:46 HGT.700.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 37977 May  3 04:46 TMP.200.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 35203 May  3 04:46 HGT.700.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 40628 May  3 04:46 TMP.200.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 48341 May  3 04:46 PRMSL.msl.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 41527 May  3 04:46 RH.700.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 48170 May  3 04:46 PRMSL.msl.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 35474 May  3 04:46 HGT.850.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 41117 May  3 04:46 UGRD.700.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 33670 May  3 04:46 HGT.500.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 39749 May  3 04:46 VVEL.200.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 42662 May  3 04:46 VGRD.850.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 44309 May  3 04:46 VGRD.300.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41467 May  3 04:46 RH.500.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 44339 May  3 04:46 VGRD.200.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 41006 May  3 04:46 VGRD.200.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 37803 May  3 04:46 TMP.300.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 36209 May  3 04:46 TMP.850.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 34565 May  3 04:46 HGT.300.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 40896 May  3 04:46 RH.300.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 33826 May  3 04:46 HGT.200.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 54478 May  3 04:46 VGRD.10m.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 49233 May  3 04:46 RH.1p5m.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 36906 May  3 04:46 VVEL.700.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 54494 May  3 04:46 VGRD.10m.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 44328 May  3 04:46 VGRD.500.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 49472 May  3 04:46 RH.1p5m.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 41570 May  3 04:46 RH.700.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 40984 May  3 04:46 RH.300.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 32213 May  3 04:46 HGT.300.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 41613 May  3 04:46 RH.500.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 35410 May  3 04:46 HGT.700.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 36994 May  3 04:46 VVEL.200.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 34997 May  3 04:46 TMP.500.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 35992 May  3 04:46 HGT.500.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 38444 May  3 04:46 VVEL.300.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 39667 May  3 04:46 VVEL.200.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 46678 May  3 04:46 VGRD.850.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 45445 May  3 04:46 TMP.1p5m.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 36204 May  3 04:46 TMP.850.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 44690 May  3 04:46 RH.300.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 33369 May  3 04:46 HGT.200.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 37016 May  3 04:46 VVEL.500.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 45231 May  3 04:46 RH.850.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 41663 May  3 04:46 UGRD.850.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 55404 May  3 04:46 UGRD.10m.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 36707 May  3 04:46 VVEL.700.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 39650 May  3 04:46 UGRD.500.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 37598 May  3 04:46 VVEL.850.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 37608 May  3 04:46 VVEL.850.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 39194 May  3 04:46 UGRD.500.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 33299 May  3 04:46 VVEL.700.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 33662 May  3 04:46 TMP.500.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 37395 May  3 04:46 VVEL.500.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 41708 May  3 04:46 UGRD.850.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 34122 May  3 04:46 TMP.500.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 36059 May  3 04:46 TMP.850.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 32450 May  3 04:46 HGT.200.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 45275 May  3 04:46 VGRD.500.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 41814 May  3 04:46 RH.700.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 46416 May  3 04:46 VGRD.850.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 45188 May  3 04:46 VGRD.500.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 35223 May  3 04:46 HGT.500.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 35452 May  3 04:46 VVEL.300.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 54954 May  3 04:46 VGRD.10m.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 38650 May  3 04:46 VVEL.850.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 40168 May  3 04:46 VVEL.200.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 38170 May  3 04:46 VVEL.300.3.2016010106.gz\n",
            "-rw-r--r--  1 root root 40960 May  3 04:46 UGRD.500.3.2016010100.gz\n",
            "-rw-r--r--  1 root root 34380 May  3 04:46 HGT.500.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 42289 May  3 04:46 RH.500.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 53695 May  3 04:46 UGRD.10m.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 45213 May  3 04:46 RH.850.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 34613 May  3 04:46 HGT.700.3.2016010118.gz\n",
            "-rw-r--r--  1 root root 54090 May  3 04:46 UGRD.10m.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 45211 May  3 04:46 RH.850.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 41829 May  3 04:46 RH.700.3.2016010112.gz\n",
            "-rw-r--r--  1 root root 33541 May  3 04:46 HGT.300.3.2016010109.gz\n",
            "-rw-r--r--  1 root root 44512 May  3 04:46 TMP.1p5m.3.2016010115.gz\n",
            "-rw-r--r--  1 root root 44460 May  3 04:46 RH.300.3.2016010103.gz\n",
            "-rw-r--r--  1 root root 44867 May  3 04:46 TMP.1p5m.3.2016010121.gz\n",
            "-rw-r--r--  1 root root 49372 May  3 04:46 RH.1p5m.3.2016010109.gz\n",
            "drwxr-xr-x  2 root root 20480 May  3 04:46 .\n",
            "drwxr-xr-x 33 root root  4096 May  3 04:46 ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "# ★24h画像 → 24h画像 推測を行う場合は ★箇所を有効に。\n",
        "#   学習用衛星画像データ　  train_sat.npy   (730,24,33,25,1)\n",
        "#   テスト用衛星画像データ  test_sat.npy    (50,24,33,25,1)\n",
        "#   学習用気象データ        train_met.npy   (730,24,33,25,34)\n",
        "#   テスト用気象データ      test_met.npy    (50,24,33,25,34)\n",
        "\n",
        "# 精度を上げるための施策\n",
        "# ・24h(1日)ではなく、96hの画像をもとに分析する\n",
        "# ・元の画像サイズを大きくする（初期は20分の1にResize）\n",
        "# ・不足している画像データの補完方法を見直し（初期は存在する前の時間をCOPY）\n",
        "\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "import cv2\n",
        "import gzip\n",
        "import shutil\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 画像の読み込み時点で評価提出時のサイズにトリミングするフラグ\n",
        "size_custom_flg = True\n",
        "# 画像、データの縮小サイズ\n",
        "reduction_size = 25  # 通常サイズをreduction_size分の1に縮小する\n",
        "\n",
        "source_folder       = '/content/sample_data/'\n",
        "destination_folder  = '/content/drive/MyDrive/2024_AIContest2/Thema2/'\n",
        "\n",
        "file_name = 'inference_terms.csv'\n",
        "csv_file_name = os.path.join(source_folder, file_name)\n",
        "\n",
        "'''\n",
        "# 風関連、上空200h,300h は除外 12個\n",
        "met_data_type_list = ['HGT.500', 'HGT.700', 'HGT.850',\n",
        "                     'PRMSL.msl',\n",
        "                     'RH.1p5m', 'RH.500', 'RH.700', 'RH.850',\n",
        "                     'TMP.1p5m', 'TMP.500', 'TMP.700', 'TMP.850']\n",
        "'''\n",
        "#SHAP分析で選択した11個\n",
        "met_data_type_list = ['HGT.500', 'HGT.700',\n",
        "                     'RH.300', 'RH.500',\n",
        "                     'TMP.1p5m', 'TMP.200', 'TMP.850',\n",
        "                     'UGRD.200',\n",
        "                     'VVEL.200', 'VVEL.300', 'VVEL.500']\n",
        "met_ch_num = len(met_data_type_list)\n",
        "'''\n",
        "# ALL\n",
        "met_data_type_list = ['HGT.200', 'HGT.300', 'HGT.500', 'HGT.700', 'HGT.850',\n",
        "                     'PRMSL.msl',\n",
        "                     'RH.1p5m', 'RH.300', 'RH.500', 'RH.700', 'RH.850',\n",
        "                     'TMP.1p5m', 'TMP.200', 'TMP.300', 'TMP.500', 'TMP.700', 'TMP.850',\n",
        "                     'UGRD.10m','UGRD.200', 'UGRD.300', 'UGRD.500', 'UGRD.700', 'UGRD.850',\n",
        "                     'VGRD.10m', 'VGRD.200', 'VGRD.300', 'VGRD.500', 'VGRD.700', 'VGRD.850',\n",
        "                     'VVEL.200', 'VVEL.300', 'VVEL.500', 'VVEL.700', 'VVEL.850']\n",
        "'''\n",
        "\n",
        "org_sat_size_h, org_sat_size_w = 672, 512 # 5キロメッシュ\n",
        "org_met_size_h, org_met_size_w = 168, 128 # 20キロメッシュ\n",
        "\n",
        "cus_sat_size_h, cus_sat_size_w = 420, 344 # 5キロメッシュ\n",
        "cus_met_size_h, cus_met_size_w =  42,  32 # 20キロメッシュ\n",
        "trim_y1, trim_y2 =  40,460\n",
        "trim_x1, trim_x2 = 128,472 # 本来は130,470だけど、metに合わせるために4の倍数\n",
        "\n",
        "# カスタムフラグ（トリミング）有無によって 元画像のサイズを設定\n",
        "#   ※気象情報のデータセット作成時もデータセットのサイズをそろえるために\n",
        "#     衛星画像のサイズを使用する。\n",
        "if size_custom_flg :\n",
        "    size_y, size_x = cus_sat_size_h, cus_sat_size_w\n",
        "else:\n",
        "    size_y, size_x = org_sat_size_h, org_sat_size_w\n",
        "\n",
        "# テスト対象期間の日付を取得する関数\n",
        "#\n",
        "def get_test_days_list():\n",
        "    # test_terms.csvからtest対象期間の日付を取得\n",
        "    test_terms = pd.read_csv(csv_file_name)\n",
        "\n",
        "    # OpenData_startカラムのデータを日付型のデータとして取得する\n",
        "    start_dates = pd.to_datetime(test_terms['OpenData_96hr_Start'])\n",
        "\n",
        "    # ★ 過去24時間の画像から24時間分の画像を推測するとき用\n",
        "    days_list = start_dates + pd.Timedelta(days=3)\n",
        "\n",
        "    return days_list\n",
        "\n",
        "# テスト対象期間の日付を取得\n",
        "#print(\"--- get_test_days_list sample---\")\n",
        "#sample_test_days_list = get_test_days_list()\n",
        "#print(sample_test_days_list[:5])\n",
        "\n",
        "# 学習対象期間の日付を取得する関数\n",
        "def get_train_days_list():\n",
        "\n",
        "    start_date = dt(2016, 1, 1, 1)\n",
        "\n",
        "    # 日付を格納するリストを初期化する\n",
        "    days_list = []\n",
        "\n",
        "    # 学習データは365日×２年分与えられるため、合計730日分の日時を取得する\n",
        "    for i in range(2*365):\n",
        "\n",
        "        # start_dateから、i日後の日時を取得し、dateに代入する\n",
        "        date = start_date + timedelta(days=i)\n",
        "\n",
        "        # days_listにdateを追加する\n",
        "        days_list.append(date)\n",
        "\n",
        "    return days_list\n",
        "\n",
        "# 関数を実行して、学習データが提供されている日の日付を取得する\n",
        "#print(\"--- get_train_days_list sample---\")\n",
        "#sample_train_days_list = get_train_days_list()\n",
        "#print(sample_train_days_list[:5])\n",
        "\n",
        "# dataの日付情報からフルパスのファイル名を返す\n",
        "def create_file_name(date):\n",
        "\n",
        "    # 日付(date)から、年(year)、月(month)、日(day)、時(hour)を数値データとして取得する\n",
        "    year  = date.year\n",
        "    month = date.month\n",
        "    day   = date.day\n",
        "    hour  = date.hour\n",
        "\n",
        "    # 2018年ならばテストデータ(test)\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    # 2016年、2017年ならば学習データ(train)\n",
        "    else:\n",
        "        phase = 'train'\n",
        "\n",
        "    # ファイル名を指定する\n",
        "    file_name = f\"{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png\"\n",
        "\n",
        "    path_file_name = os.path.join(source_folder, file_name)\n",
        "\n",
        "    return path_file_name\n",
        "\n",
        "def get_sat_data(date):\n",
        "\n",
        "    # 衛星画像はファイルが存在しない時間(日)があるので、\n",
        "    # ない場合は前の時間の画像を使う。\n",
        "    file_exists = False\n",
        "    while not file_exists:\n",
        "        full_file_name = create_file_name(date)\n",
        "        file_exists = os.path.exists(full_file_name)\n",
        "        if not file_exists:\n",
        "            date = date - timedelta(hours=1)\n",
        "            print(f\"Warning: File not found: {full_file_name}\")\n",
        "            print(f\"Retry with date: {date}\")\n",
        "\n",
        "    # 画像の読み込み\n",
        "    sat_data = cv2.imread(full_file_name, 0)\n",
        "\n",
        "    # カスタムフラグ（トリミング）TRUE時は カスタムサイズでトリミング\n",
        "    if size_custom_flg :\n",
        "        img_resize = sat_data[trim_y1:trim_y2, trim_x1:trim_x2]\n",
        "        return img_resize\n",
        "    # トリミングなし\n",
        "    else:\n",
        "        return sat_data\n",
        "\n",
        "# 「2016年1月1日16時」時点の衛星画像を読み込む\n",
        "#print(\"--- get_sat_data sample---\")\n",
        "#sat_data = get_sat_data(dt(2016,1,1,1))\n",
        "#print(sat_data.shape)\n",
        "#print(sat_data)\n",
        "\n",
        "# 気象データのgzを読み込んで配列で返す\n",
        "def Read_gz_Binary(file_path):\n",
        "\n",
        "    # 1. 変数file_tmpに「元のファイル名_tmp」となる文字列を代入\n",
        "    file_tmp = file_path + \"_tmp\"\n",
        "\n",
        "    # 2. gzipファイルを、「読み込み(r)モード & バイナリ(b)モード」 で開き、\n",
        "    #     開いたファイルオブジェクトをf_inに代入する\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "\n",
        "        # 3. ファイル名を「元のファイル名_tmp」としたファイルを\n",
        "        #     open()関数の「書き込み(w)モード & バイナリ(b)モード」で新規作成し、\n",
        "        #     ファイルオブジェクトをf_outとする\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "\n",
        "            # 4. f_inのファイルオブジェクトをf_outにコピーする\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    # 5. バイナリファイルをNumPy配列として読み込む\n",
        "    met_data = np.fromfile(file_tmp, np.float32)\n",
        "\n",
        "    # 6. 一時的に作成した展開済みのバイナリファイル「元のファイル名_tmp」を削除する\n",
        "    os.remove(file_tmp)\n",
        "\n",
        "    # 7. 読み込んだ配列の形状をOriginaの気象データサイズに整形する\n",
        "    #    1次元の配列を2次元に整形\n",
        "    met_data = met_data.reshape( [org_met_size_h, org_met_size_w] )\n",
        "\n",
        "    # カスタムフラグ（トリミング）TRUE時は カスタムサイズでトリミング\n",
        "    if size_custom_flg :\n",
        "#        met_resize = met_data[ (trim_y1/4):(trim_y2/4), (trim_x1/4):(trim_x2/4)]\n",
        "        met_resize = met_data[ 10:115, 32:118]\n",
        "        return met_resize\n",
        "    # トリミングなし\n",
        "    else:\n",
        "        return met_data\n",
        "\n",
        "# HGT.200.3.2016010100.gz の気象データを確認\n",
        "#print(\"--- Read_gz_Binary sample---\")\n",
        "#tmp_file_name = '/content/sample_data/train/met/2016/01/01/HGT.200.3.2016010100.gz'\n",
        "#tmp_array = Read_gz_Binary(tmp_file_name)\n",
        "#print(tmp_array.shape)\n",
        "#print(tmp_array)\n",
        "\n",
        "## 気象データの欠損部分を補完する。\n",
        "def fill_lack_data(data):\n",
        "\n",
        "    # カスタムフラグ（トリミング）FALSE時は 補完を実施\n",
        "    if not size_custom_flg  :\n",
        "        ## 1. 北側、南側の未計測部分を補間する(上下)\n",
        "        # 北側の未計測部分を補間する\n",
        "        data[0:2] = data[2]\n",
        "        # 南側の未計測部分を補間する\n",
        "        data[154:] = data[153]\n",
        "\n",
        "        ## 2. 西側の未計測部分を補間する(左右)\n",
        "\n",
        "        # 西側の未計測部分を補間する\n",
        "        data[:, :8] = data[:,8].reshape(-1, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 気象データを読み込むライブラリ\n",
        "def get_met_data(date, data_type):\n",
        "\n",
        "    # 日付(date)から、年(year)、月(month)、日(day)、時(hour)を数値データとして取得する\n",
        "    year  = date.year\n",
        "    month = date.month\n",
        "    day  = date.day\n",
        "    hour = date.hour\n",
        "\n",
        "    # 2018年ならばテストデータ(test)\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    # 2016年、2017年ならば学習データ(train)\n",
        "    else:\n",
        "        phase = 'train'\n",
        "\n",
        "    # ファイル名を指定する\n",
        "    file_name = f'{phase}/met/{year}/{month:02}/{day:02}/{data_type}.3.{year}{month:02}{day:02}{hour:02}.gz'\n",
        "\n",
        "    full_file_name = os.path.join(source_folder, file_name)\n",
        "\n",
        "    # データの読み込み(Read_gz_Binary) -> 空白箇所の穴埋め(fill_lack_data)の順番で処理を行う\n",
        "    met_data = fill_lack_data(Read_gz_Binary(full_file_name))\n",
        "\n",
        "    return met_data\n",
        "\n",
        "# 衛星画像のデータセットを作成\n",
        "def make_sat_dataset(resize, phase):\n",
        "\n",
        "    # 引数phaseの値に従って、\n",
        "    # 学習用、もしくはテスト用のデータが与えられている日付を取得する\n",
        "    ## [get_train_days_list()、get_test_days_list()はともにタスク1で作成した関数]\n",
        "    if phase == 'train':\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    # データセットを格納するリストの初期化\n",
        "    dataset = []\n",
        "\n",
        "    # start_date_listから、順番にデータが与えられている日付を取得し、\n",
        "    # 変数start_dateに代入する\n",
        "    for start_date in start_date_list:\n",
        "\n",
        "        # 一日分の衛星画像データを格納するリストの初期化\n",
        "        data_in_1day = []\n",
        "\n",
        "        # 24時間分の衛星画像を順番に取得する\n",
        "        for i in range(24):\n",
        "            # start_dateから、時間を「i時間」分進める\n",
        "            date = start_date + timedelta(hours=i)\n",
        "\n",
        "            # 与えられた日時(date)における衛星画像データを読み込む\n",
        "            ## [get_sat_data()関数は当タスク内で作成した関数]\n",
        "            sat_data = get_sat_data(date)\n",
        "\n",
        "            # 引数resizeの値に従って、データを縮小する\n",
        "            #   cv2.resize(入力画像, dsize(横,縦),interpolation)\n",
        "            resized_data = cv2.resize(sat_data, ( int(size_x / resize), int(size_y / resize) ),\n",
        "                                                          interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # data_in_1dayリストへの値の追加\n",
        "            data_in_1day.append(resized_data)\n",
        "\n",
        "        # 1日分(24時間分)のデータを格納したリストをNumPy配列に変換する\n",
        "        # 形状は(24, 高さ, 幅, 1)\n",
        "        data_in_1day = np.array(data_in_1day, dtype='uint8').reshape(24, int(size_y / resize), int(size_x / resize), 1)\n",
        "\n",
        "        # 取得した1日分のNumPy配列をdatasetリストに追加する\n",
        "        dataset.append(data_in_1day)\n",
        "\n",
        "    # 対象期間に含まれる全データを格納したリストをNumPy配列に変換する\n",
        "    # 形状は(対象期間の日数, 24, 高さ, 幅, 1)\n",
        "    dataset = np.array(dataset, dtype='uint8').reshape(len(start_date_list), 24, int(size_y / resize), int(size_x / resize), 1)\n",
        "\n",
        "    # 保存するnpyファイル名を指定する\n",
        "    save_name = f'{phase}_sat_{size_custom_flg}_{resize}.npy'\n",
        "\n",
        "    full_save_name = os.path.join(destination_folder, save_name)\n",
        "\n",
        "    # NumPy配列をnpyファイルとして保存する\n",
        "    np.save(full_save_name, dataset)\n",
        "\n",
        "# 気象データのデータセットを作成\n",
        "def make_met_dataset(resize, phase, data_type_list):\n",
        "\n",
        "    if phase == 'train':\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    # 全種類、全日時分のデータを格納するリストの初期化\n",
        "    dataset = []\n",
        "\n",
        "    for start_date in start_date_list:\n",
        "\n",
        "        #print(start_date)\n",
        "        # 引数data_type_listから、\n",
        "        # 気象データの種類を示す文字列を1つずつ取り出す。\n",
        "        for count, data_type in enumerate(data_type_list):\n",
        "\n",
        "            # 1種類、1日分のデータを格納するリストの初期化\n",
        "            one_type_in_1day = []\n",
        "\n",
        "            for i in range(24):\n",
        "\n",
        "                date = start_date + timedelta(hours=i)\n",
        "                hour = date.hour\n",
        "\n",
        "                \"\"\"データ補間処理の方針\n",
        "                A:\n",
        "                3で割り切ることのできる時刻T(0時, 3時, 6時, 9時, 12時, 15時, 18時, 21時)は、\n",
        "                気象データが提供されているため、単純にデータを読み込む。\n",
        "\n",
        "                B:\n",
        "                それ以外の時刻T+1とT+2については、\n",
        "                時刻Tと時刻T+3時点の気象データを利用して、値を補間する。\n",
        "\n",
        "                \"\"\"\n",
        "\n",
        "                # 時刻がT[0, 3, 6, 9, 12, 15, 18, 21]時の場合\n",
        "                if hour % 3 == 0:\n",
        "\n",
        "                    met_data = get_met_data(date, data_type)\n",
        "\n",
        "                # 時刻がT+1[1, 4, 7, 10, 13, 16, 19, 22]時の場合\n",
        "                elif hour % 3 == 1:\n",
        "\n",
        "                    # １時間前(時刻T)の気象データを読み込む\n",
        "                    before_1h = date - timedelta(hours=1)\n",
        "                    data_before_1h = get_met_data(before_1h, data_type)\n",
        "\n",
        "                    # ２時間後(時刻T+3)の気象データを読み込む\n",
        "                    after_2h = date + timedelta(hours=2)\n",
        "                    data_after_2h = get_met_data(after_2h, data_type)\n",
        "\n",
        "                    # 前後の値を利用して、補間する\n",
        "                    met_data = (2/3) * data_before_1h + (1/3) * data_after_2h\n",
        "\n",
        "                # 時刻がT+2[2, 5, 8, 11, 14, 17, 20, 23]時の場合\n",
        "                else:\n",
        "\n",
        "                    # ２時間前(時刻T)の気象データを読み込む\n",
        "                    before_2h = date - timedelta(hours=2)\n",
        "                    data_before_2h = get_met_data(before_2h, data_type)\n",
        "\n",
        "                    # １時間後(時刻T+3)の気象データを読み込む\n",
        "                    after_1h = date + timedelta(hours=1)\n",
        "                    data_after_1h = get_met_data(after_1h, data_type)\n",
        "\n",
        "                    # 前後の値を利用して、補間する\n",
        "                    met_data = (1/3) * data_before_2h + (2/3) * data_after_1h\n",
        "\n",
        "                # 引数resizeの値に従って、データを縮小する\n",
        "                resized_data = cv2.resize(met_data, (int(size_x / resize), int(size_y / resize)),\n",
        "                                                              interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                # 1種類、1時刻分のデータをone_type_in_1dayリストへ追加する\n",
        "                one_type_in_1day.append(resized_data)\n",
        "\n",
        "            one_type_in_1day = np.array(one_type_in_1day).reshape(24, int(size_y / resize), int(size_x / resize), 1)\n",
        "\n",
        "            # 1種類目はall_type_in_1dayに代入する\n",
        "            if count == 0:\n",
        "                all_type_in_1day = one_type_in_1day\n",
        "            # 2種類目以降はall_type_in_1dayにチャンネルの次元で結合する\n",
        "            else:\n",
        "                all_type_in_1day = np.concatenate([all_type_in_1day, one_type_in_1day], axis=3)\n",
        "\n",
        "        # 全種類、1日分のデータをdatasetリストへ追加する\n",
        "        dataset.append(all_type_in_1day)\n",
        "\n",
        "    dataset = np.array(dataset, dtype='float32').reshape(len(start_date_list), 24, int(size_y / resize), int(size_x / resize), len(data_type_list))\n",
        "\n",
        "    save_name = f'{phase}_met{met_ch_num}_{size_custom_flg}_{resize}.npy'\n",
        "    full_save_name = os.path.join(destination_folder, save_name)\n",
        "    np.save(full_save_name, dataset)\n",
        "\n",
        "print ('---- make_sat_dataset train ----')\n",
        "make_sat_dataset(resize=reduction_size, phase='train')\n",
        "print ('---- make_sat_dataset test ----')\n",
        "make_sat_dataset(resize=reduction_size, phase='test')\n",
        "print ('---- make_met_dataset train ----')\n",
        "make_met_dataset(resize=reduction_size, phase='train', data_type_list=met_data_type_list)\n",
        "print ('---- make_met_dataset test ----')\n",
        "make_met_dataset(resize=reduction_size, phase='test', data_type_list=met_data_type_list)"
      ],
      "metadata": {
        "id": "SJF3U1g6isHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec793d97-8557-43ea-842f-967038c2b35b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---- make_sat_dataset train ----\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-01-22/2016-01-22-21-00.fv.png\n",
            "Retry with date: 2016-01-22 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-07/2016-02-07-21-00.fv.png\n",
            "Retry with date: 2016-02-07 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-15/2016-02-15-15-00.fv.png\n",
            "Retry with date: 2016-02-15 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-15/2016-02-15-16-00.fv.png\n",
            "Retry with date: 2016-02-15 15:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-15/2016-02-15-15-00.fv.png\n",
            "Retry with date: 2016-02-15 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-15/2016-02-15-17-00.fv.png\n",
            "Retry with date: 2016-02-15 16:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-15/2016-02-15-16-00.fv.png\n",
            "Retry with date: 2016-02-15 15:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-15/2016-02-15-15-00.fv.png\n",
            "Retry with date: 2016-02-15 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-22/2016-02-22-21-00.fv.png\n",
            "Retry with date: 2016-02-22 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-02-29/2016-02-29-22-00.fv.png\n",
            "Retry with date: 2016-02-29 21:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-03-07/2016-03-07-21-00.fv.png\n",
            "Retry with date: 2016-03-07 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-03-14/2016-03-14-21-00.fv.png\n",
            "Retry with date: 2016-03-14 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-03-28/2016-03-28-20-00.fv.png\n",
            "Retry with date: 2016-03-28 19:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-03-31/2016-03-31-18-00.fv.png\n",
            "Retry with date: 2016-03-31 17:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-04-01/2016-04-01-06-00.fv.png\n",
            "Retry with date: 2016-04-01 05:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-04-11/2016-04-11-19-00.fv.png\n",
            "Retry with date: 2016-04-11 18:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-04-28/2016-04-28-09-00.fv.png\n",
            "Retry with date: 2016-04-28 08:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-04-28/2016-04-28-21-00.fv.png\n",
            "Retry with date: 2016-04-28 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-07-04/2016-07-04-13-00.fv.png\n",
            "Retry with date: 2016-07-04 12:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-07-18/2016-07-18-13-00.fv.png\n",
            "Retry with date: 2016-07-18 12:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-08-18/2016-08-18-10-00.fv.png\n",
            "Retry with date: 2016-08-18 09:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-08-18/2016-08-18-22-00.fv.png\n",
            "Retry with date: 2016-08-18 21:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-09-01/2016-09-01-20-00.fv.png\n",
            "Retry with date: 2016-09-01 19:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-09-02/2016-09-02-08-00.fv.png\n",
            "Retry with date: 2016-09-02 07:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-10-27/2016-10-27-20-00.fv.png\n",
            "Retry with date: 2016-10-27 19:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-10-28/2016-10-28-08-00.fv.png\n",
            "Retry with date: 2016-10-28 07:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-11-21/2016-11-21-04-00.fv.png\n",
            "Retry with date: 2016-11-21 03:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-11-24/2016-11-24-11-00.fv.png\n",
            "Retry with date: 2016-11-24 10:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-11-24/2016-11-24-23-00.fv.png\n",
            "Retry with date: 2016-11-24 22:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-12-05/2016-12-05-04-00.fv.png\n",
            "Retry with date: 2016-12-05 03:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-12-08/2016-12-08-21-00.fv.png\n",
            "Retry with date: 2016-12-08 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2016-12-09/2016-12-09-09-00.fv.png\n",
            "Retry with date: 2016-12-09 08:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-01-05/2017-01-05-19-00.fv.png\n",
            "Retry with date: 2017-01-05 18:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-01-06/2017-01-06-07-00.fv.png\n",
            "Retry with date: 2017-01-06 06:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-01-22/2017-01-22-21-00.fv.png\n",
            "Retry with date: 2017-01-22 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-02-07/2017-02-07-21-00.fv.png\n",
            "Retry with date: 2017-02-07 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-02-13/2017-02-13-15-00.fv.png\n",
            "Retry with date: 2017-02-13 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-02-13/2017-02-13-16-00.fv.png\n",
            "Retry with date: 2017-02-13 15:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-02-13/2017-02-13-15-00.fv.png\n",
            "Retry with date: 2017-02-13 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-02-22/2017-02-22-21-00.fv.png\n",
            "Retry with date: 2017-02-22 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-03-02/2017-03-02-08-00.fv.png\n",
            "Retry with date: 2017-03-02 07:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-03-02/2017-03-02-20-00.fv.png\n",
            "Retry with date: 2017-03-02 19:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-03-07/2017-03-07-21-00.fv.png\n",
            "Retry with date: 2017-03-07 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-04-13/2017-04-13-15-00.fv.png\n",
            "Retry with date: 2017-04-13 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-04-14/2017-04-14-03-00.fv.png\n",
            "Retry with date: 2017-04-14 02:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-05-25/2017-05-25-12-00.fv.png\n",
            "Retry with date: 2017-05-25 11:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-05-26/2017-05-26-00-00.fv.png\n",
            "Retry with date: 2017-05-25 23:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-06-08/2017-06-08-15-00.fv.png\n",
            "Retry with date: 2017-06-08 14:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-06-09/2017-06-09-03-00.fv.png\n",
            "Retry with date: 2017-06-09 02:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-06-22/2017-06-22-00-00.fv.png\n",
            "Retry with date: 2017-06-21 23:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-06-22/2017-06-22-12-00.fv.png\n",
            "Retry with date: 2017-06-22 11:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-07-03/2017-07-03-14-00.fv.png\n",
            "Retry with date: 2017-07-03 13:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-08-03/2017-08-03-21-00.fv.png\n",
            "Retry with date: 2017-08-03 20:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-08-04/2017-08-04-09-00.fv.png\n",
            "Retry with date: 2017-08-04 08:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-08-17/2017-08-17-18-00.fv.png\n",
            "Retry with date: 2017-08-17 17:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-08-18/2017-08-18-06-00.fv.png\n",
            "Retry with date: 2017-08-18 05:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-11-20/2017-11-20-05-00.fv.png\n",
            "Retry with date: 2017-11-20 04:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-12-04/2017-12-04-04-00.fv.png\n",
            "Retry with date: 2017-12-04 03:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-12-07/2017-12-07-22-00.fv.png\n",
            "Retry with date: 2017-12-07 21:00:00\n",
            "Warning: File not found: /content/sample_data/train/sat/2017-12-08/2017-12-08-10-00.fv.png\n",
            "Retry with date: 2017-12-08 09:00:00\n",
            "---- make_sat_dataset test ----\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-15/2018-06-15-21-00.fv.png\n",
            "Retry with date: 2018-06-15 20:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-30/2018-06-30-09-00.fv.png\n",
            "Retry with date: 2018-06-30 08:00:00\n",
            "---- make_met_dataset train ----\n",
            "---- make_met_dataset test ----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -fr /content/my_model_*\n"
      ],
      "metadata": {
        "id": "3RVAIY03KDMU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデリング (train) / 評価 (test)\n",
        "\n",
        "# 改善案)\n",
        "# ・正規化の時にfloat64で正規化\n",
        "# ・クロスバリデーション実施\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D,TimeDistributed\n",
        "from tensorflow.keras.layers import Conv2D,Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/2024_AIContest2/Thema2/'\n",
        "\n",
        "train_sat_name = os.path.join(source_folder, 'train_sat_False_15.npy')\n",
        "test_sat_name  = os.path.join(source_folder,  'test_sat_False_15.npy')\n",
        "train_met_name = os.path.join(source_folder, 'train_met12_False_20.npy')\n",
        "test_met_name  = os.path.join(source_folder,  'test_met12_False_20.npy')\n",
        "best_h5        = 'my_model_Epoch139_loss0.0799_valloss0.0820.weights.h5'\n",
        "best_h5_name   = os.path.join(source_folder, best_h5)\n",
        "\n",
        "mode            = 'train'# 学習時は「train」評価は「test」\n",
        "size_custom_flg = False   # 画像の読み込み時点で評価提出時のサイズにトリミングするフラグ\n",
        "merge_flag      = False  # True時、気象データを使用する\n",
        "reduction_size  = 15     # 画像、気象データの縮小サイズ 通常サイズをreduction_size分の1に縮小する\n",
        "filter_size     = 20     # モデルパラメータ フィルタサイズ\n",
        "met_ch_size     = 12     # met:12\n",
        "batch_size      = 16\n",
        "num_of_epoch    = 200\n",
        "\n",
        "\n",
        "### サイズの定義  ###\n",
        "org_sat_size_h, org_sat_size_w = 672, 512 # 5キロメッシュ\n",
        "org_met_size_h, org_met_size_w = 168, 128 # 20キロメッシュ\n",
        "\n",
        "cus_sat_size_h, cus_sat_size_w = 420, 344 # 5キロメッシュ\n",
        "cus_met_size_h, cus_met_size_w =  42,  32 # 20キロメッシュ\n",
        "\n",
        "if size_custom_flg :\n",
        "    size_y, size_x = cus_sat_size_h, cus_sat_size_w\n",
        "else:\n",
        "    size_y, size_x = org_sat_size_h, org_sat_size_w\n",
        "\n",
        "# EncoderのINPUTデータのチャネルサイズ\n",
        "# 気象データありの場合は 衛星画像CH(1)＋気象データCHとなる。\n",
        "if merge_flag :\n",
        "    input_ch_size = 1 + met_ch_size\n",
        "else:\n",
        "    input_ch_size = 1\n",
        "\n",
        "###------ モデルの作成 --------###\n",
        "# モデルを初期化する\n",
        "model = Sequential()\n",
        "\n",
        "# Encoder 追加\n",
        "# 改善POINT filters、kernel_size\n",
        "model.add(ConvLSTM2D(\n",
        "  filters=filter_size, kernel_size=(3, 3),\n",
        "  padding='same', return_sequences=False,\n",
        "  activation='tanh', recurrent_activation='sigmoid',\n",
        "  input_shape=(24, int(size_y / reduction_size), int(size_x / reduction_size),input_ch_size)))\n",
        "\n",
        "# EncoderとDecoderをつなぐ中間部分 追加\n",
        "def repeat_last_status(x):\n",
        "    x = tf.reshape(x, (-1, 1, int(size_y / reduction_size), int(size_x / reduction_size), filter_size))\n",
        "    copied_x = tf.identity(x)\n",
        "    for _ in range(23):\n",
        "        x = tf.concat([x, copied_x], axis=1)\n",
        "    return x\n",
        "model.add(Lambda(repeat_last_status))\n",
        "\n",
        "# Decoder 追加\n",
        "model.add(ConvLSTM2D(\n",
        "  filters=filter_size, kernel_size=(3, 3),\n",
        "  padding='same', return_sequences=True,\n",
        "  activation='tanh', recurrent_activation='sigmoid'))\n",
        "\n",
        "# 出力層 追加\n",
        "model.add(TimeDistributed(Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')))\n",
        "\n",
        "# モデルをコンパイルする\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "###------ モデルの作成 END --------###\n",
        "\n",
        "###------ 気象情報の正規化 関数定義 --------###\n",
        "def get_met_max_min(dataset):\n",
        "    # 気象データのチャンネル数を取得する 例) (730, 24, 33, 25, 12)\n",
        "    n_data_type = dataset.shape[4]\n",
        "\n",
        "    max_array = []\n",
        "    min_array = []\n",
        "\n",
        "    # チャネル数分のインデックスを順番に取得する\n",
        "    for i in range(n_data_type):\n",
        "\n",
        "        # チャンネルの次元をインデックスiで指定し、\n",
        "        # チャンネルごとの最大値、最小値を取得する\n",
        "        max_value = np.max(dataset[:,:,:,:,i])\n",
        "        min_value = np.min(dataset[:,:,:,:,i])\n",
        "\n",
        "        max_array.append(max_value)\n",
        "        min_array.append(min_value)\n",
        "\n",
        "    max_array = np.array(max_array)\n",
        "    min_array = np.array(min_array)\n",
        "\n",
        "    return max_array, min_array\n",
        "\n",
        "def normalize_met_data(met_data, max_array, min_array):\n",
        "\n",
        "    #\n",
        "    n_data_type = met_data.shape[4]\n",
        "\n",
        "    for i in range(n_data_type):\n",
        "        # max_array, min_arrayを利用して、正規化を実行する\n",
        "        met_data[:, :, :, :, i] = (met_data[:, :, :, :, i] - min_array[i]) / (max_array[i] - min_array[i])\n",
        "\n",
        "    return met_data\n",
        "\n",
        "###------ 気象情報の正規化 関数定義 END --------###\n",
        "\n",
        "###------ 学習／評価 本番処理 --------###\n",
        "if mode == 'train':\n",
        "    ### データ前処理 ###\n",
        "    # npyを読み込む\n",
        "    train_sat_dataset = np.load(train_sat_name)\n",
        "    print('-- 学習用 衛星画像 データセットサイズ --')\n",
        "    print(train_sat_dataset.shape)\n",
        "\n",
        "    print('衛星画像データの学習用データ、検証用データを抽出する')\n",
        "    # 学習用データ、検証用データを抽出する\n",
        "    train_sat_data = train_sat_dataset[:365]\n",
        "    val_sat_data   = train_sat_dataset[365:]\n",
        "\n",
        "    print('衛星画像データを正規化する')\n",
        "    # 衛星画像データを正規化する\n",
        "    train_sat_data = train_sat_data.astype(np.float32) / 255\n",
        "    val_sat_data   = val_sat_data.astype(np.float32)   / 255\n",
        "\n",
        "    if merge_flag :\n",
        "        train_met_dataset = np.load(train_met_name)\n",
        "        print('-- 学習用 気象データ データセットサイズ --')\n",
        "        print(train_met_dataset.shape)\n",
        "\n",
        "        print('気象データの学習用データ、検証用データを抽出する')\n",
        "        # 学習用データ、検証用データを抽出する\n",
        "        train_met_data = train_met_dataset[:365]\n",
        "        val_met_data   = train_met_dataset[365:]\n",
        "\n",
        "        # 最大・最小値の取得\n",
        "        max_array, min_array = get_met_max_min(dataset=train_met_data)\n",
        "\n",
        "        # 評価(test)時に使用するため、ファイルにSaveしておく。\n",
        "        max_save_name = f'max_array_{met_ch_size}ch.npy'\n",
        "        min_save_name = f'min_array_{met_ch_size}ch.npy'\n",
        "        met_max_fname = os.path.join(source_folder, max_save_name)\n",
        "        met_min_fname = os.path.join(source_folder, min_save_name)\n",
        "        np.save(met_max_fname, max_array)\n",
        "        np.save(met_min_fname, min_array)\n",
        "\n",
        "        # 正規化実施\n",
        "        print('気象データ trainを正規化する')\n",
        "        train_met_data = normalize_met_data(met_data=train_met_data,\n",
        "                                               max_array=max_array, min_array=min_array)\n",
        "\n",
        "        print('気象データ valを正規化する')\n",
        "        # スケールの基準はtrainと同じにしておく必要がある。\n",
        "        # モデルが本来知るべきでない未来の情報を学習に取り込むことになるため\n",
        "        val_met_data = normalize_met_data(met_data=val_met_data,\n",
        "                                               max_array=max_array, min_array=min_array)\n",
        "\n",
        "        # 衛星画像と気象情報をマージ\n",
        "        train_data = np.concatenate([train_sat_data, train_met_data], axis=4)\n",
        "        val_data   = np.concatenate([val_sat_data, val_met_data], axis=4)\n",
        "\n",
        "    else:\n",
        "        train_data = train_sat_data\n",
        "        val_data   = val_sat_data\n",
        "\n",
        "    print('-- 学習用 データセットサイズ --')\n",
        "    print(train_data.shape)\n",
        "    print(val_data.shape)\n",
        "\n",
        "    print('入力データと正解データを作る → 次の1日分が正解データ')\n",
        "    # 入力データと正解データを作る → 次の1日分が正解データ\n",
        "    X_index = np.arange(364)   # 0-363(364個) のIndex作成\n",
        "    Y_index = np.arange(1,365) # 1-364(364個) のIndex作成\n",
        "\n",
        "    print('データ作成')\n",
        "    X_train = train_data[X_index]\n",
        "    Y_train = train_sat_data[Y_index] # 予測対象は衛星画像のみ\n",
        "    X_val   = val_data[X_index]\n",
        "    Y_val   = val_sat_data[Y_index]   # 予測対象は衛星画像のみ\n",
        "\n",
        "    print('モデルを使って学習実施')\n",
        "    ### モデルを使って学習実施 ###\n",
        "    # 検証用データをタプルにまとめる\n",
        "    validation_data = (X_val, Y_val)\n",
        "\n",
        "    # EarlyStoppingの設定\n",
        "    e_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "\n",
        "    # Checkpointの設定\n",
        "    checkpoint = ModelCheckpoint(filepath='my_model_Epoch{epoch}_loss{loss:.4f}_valloss{val_loss:.4f}.weights.h5',\n",
        "                                monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    # callbacksをリストにまとめる\n",
        "    callbacks = [e_stopping, checkpoint]\n",
        "\n",
        "    # 学習を実行する\n",
        "    history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_of_epoch,\n",
        "                    validation_data=validation_data, callbacks=callbacks)\n",
        "\n",
        "    # 結果をグラフで出力\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    ### データ前処理 ###\n",
        "    # npyを読み込む\n",
        "    test_sat_dataset = np.load(test_sat_name)\n",
        "    print('test_sat_dataset データサイズ')\n",
        "    print(test_sat_dataset.shape)\n",
        "\n",
        "    # 前処理 正規化\n",
        "    print('test_sat_dataset 正規化実施')\n",
        "    test_sat_dataset = test_sat_dataset.astype(np.float32) / 255\n",
        "\n",
        "    if merge_flag :\n",
        "        # 気象データをマージ\n",
        "        # npyを読み込む\n",
        "        test_met_dataset = np.load(test_met_name)\n",
        "        print('test_met_dataset データサイズ')\n",
        "        print(test_met_dataset.shape)\n",
        "\n",
        "        print('衛星画像データを正規化する')\n",
        "        # 最大・最小値の取得\n",
        "        max_save_name = f'max_array_{met_ch_size}ch.npy'\n",
        "        min_save_name = f'min_array_{met_ch_size}ch.npy'\n",
        "        met_max_fname = os.path.join(source_folder, max_save_name)\n",
        "        met_min_fname = os.path.join(source_folder, min_save_name)\n",
        "        max_array = np.load(met_max_fname)\n",
        "        min_array = np.load(met_min_fname)\n",
        "\n",
        "        # 正規化実施\n",
        "        test_met_dataset = normalize_met_data(met_data=test_met_dataset,\n",
        "                                               max_array=max_array, min_array=min_array)\n",
        "        # 衛星画像と気象情報をマージ\n",
        "        X_test = np.concatenate([test_sat_dataset, test_met_dataset], axis=4)\n",
        "\n",
        "    else:\n",
        "        X_test = test_sat_dataset\n",
        "\n",
        "    print('test_dataset サイズ')\n",
        "    print(X_test.shape)\n",
        "\n",
        "    model.load_weights(best_h5_name)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = Y_pred * 255.0\n",
        "    Y_pred = np.round(Y_pred)\n",
        "\n",
        "    #np.save('predicted_satellite.npy', Y_pred)\n",
        "\n",
        "    # 評価結果を出力\n",
        "    # 出力サイズの定義\n",
        "    # データセットのタイミングでトリミングあり\n",
        "    if size_custom_flg :\n",
        "        resized_h, resized_w = cus_sat_size_h, cus_sat_size_w # Resize前のサイズ\n",
        "        crop_y1, crop_y2     =  0, 420    # データセットのタイミングでトリミング済み\n",
        "        crop_x1, crop_x2     =  2, 342    # データセットのトリミングから、両端2グリッドずつトリミング\n",
        "    # トリミングなし\n",
        "    else:\n",
        "        resized_h, resized_w = org_sat_size_h, org_sat_size_w # Resize前のサイズ\n",
        "        crop_y1, crop_y2     =  40, 460   # 縦方向 切り出し\n",
        "        crop_x1, crop_x2     = 130, 470   # 横方向 切り出し\n",
        "\n",
        "    target_hours         = [5, 11, 17, 23]  # 時刻インデックス\n",
        "\n",
        "    # 各画像サイズ：420x340、200枚 → 全体サイズ：(420*200, 340)\n",
        "    cropped_height = crop_y2 - crop_y1  # 420\n",
        "    cropped_width  = crop_x2 - crop_x1  # 340\n",
        "    total_images   = 50 * 4             # 200枚\n",
        "\n",
        "    # 空の大きな配列（縦長）を作成\n",
        "    vertical_concat_image = np.zeros((total_images * cropped_height, cropped_width), dtype=np.uint8)\n",
        "\n",
        "    # 画像処理と縦方向への結合\n",
        "    row_pos = 0\n",
        "    for day in range(50):\n",
        "        for hour in target_hours:\n",
        "            img_small = Y_pred[day, hour, :, :, 0]  # (33, 25)\n",
        "            img_resized = cv2.resize(img_small, (resized_w, resized_h), interpolation=cv2.INTER_LINEAR)\n",
        "            cropped_img = img_resized[crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "            cropped_img_uint8 = np.clip(cropped_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "            vertical_concat_image[row_pos:row_pos + cropped_height, :] = cropped_img_uint8\n",
        "            row_pos += cropped_height\n",
        "\n",
        "    # DataFrameに変換（Index付き）\n",
        "    df = pd.DataFrame(vertical_concat_image)\n",
        "    df.index = np.arange(vertical_concat_image.shape[0])  # 0〜83999\n",
        "\n",
        "    save_file_name = 'submit_image_with_index.csv'\n",
        "    save_file_path = os.path.join(source_folder, save_file_name)\n",
        "\n",
        "    # CSVとして保存（行番号付き）\n",
        "    df.to_csv(save_file_path, header=False, index=True)\n",
        "    print('-- submit_image_with_index.csv Saved!')\n",
        "\n"
      ],
      "metadata": {
        "id": "DMIpsh6PT8GR",
        "outputId": "6d3897fe-1a91-4b87-95b4-f539dba0322a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "incomplete input (<ipython-input-8-4382390dc020>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-4382390dc020>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HGT: PMMSL:海面気温 RH:湿度 TMP:気温 UGRD:東西風 VGRD:南北風 VVEL:鉛直流\n",
        "\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import gzip\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "# 気象データを読み込む関数; Read_gz_Binary を実装する\n",
        "def Read_gz_Binary(file_path):\n",
        "    file_tmp = file_path + \"_tmp\"\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    # バイナリファイルをNumPy配列として読み込む\n",
        "    met_data = np.fromfile(file_tmp, np.float32)\n",
        "    os.remove(file_tmp)\n",
        "\n",
        "    # 配列の次元に変更を加える\n",
        "    met_data = met_data.reshape( [168,128] )\n",
        "\n",
        "    return met_data\n",
        "\n",
        "# 気象データのファイルパスを指定する\n",
        "file_path = '/content/sample_data/train/met/2016/01/02/TMP.1p5m.3.2016010218.gz'\n",
        "\n",
        "# 気象データを読み込む\n",
        "met_data = Read_gz_Binary(file_path)\n",
        "\n",
        "# 読み込んだ気象データに関する基本情報を出力する\n",
        "print('気象データに関する基本情報')\n",
        "print(type(met_data))\n",
        "print(met_data.shape)\n",
        "\n",
        "# 最大値の出力\n",
        "print('最大値: ', np.max(met_data))\n",
        "# 最小値の出力\n",
        "print('最小値: ', np.min(met_data))\n",
        "# 平均値の出力\n",
        "print('平均値: ', np.mean(met_data))\n",
        "\n",
        "#print(met_data)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(met_data, 'gray')\n",
        "\n",
        "# fill_lack_data()関数を実装する\n",
        "def fill_lack_data(data):\n",
        "    ## 1. 北側、南側の未計測部分を補間する(上下)\n",
        "    # 北側の未計測部分を補間する\n",
        "    data[0:2] = data[2]\n",
        "    # 南側の未計測部分を補間する\n",
        "    data[154:] = data[153]\n",
        "    ## 2. 西側の未計測部分を補間する(左右)\n",
        "    # 西側の未計測部分を補間する\n",
        "    data[:, :8] = data[:,8].reshape(-1, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# fill_lack_data()関数をmet_dataに対して実行する\n",
        "filled_data = fill_lack_data(met_data)\n",
        "\n",
        "# 最大値の出力\n",
        "print('最大値: ', np.max(met_data))\n",
        "# 最小値の出力\n",
        "print('最小値: ', np.min(met_data))\n",
        "# 平均値の出力\n",
        "print('平均値: ', np.mean(met_data))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(filled_data, 'gray')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kg6HUS6_xRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# npyファイルの画像を表示\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/2024_AIContest2/Thema2/'\n",
        "\n",
        "# npyを読み込む\n",
        "train_sat_name = os.path.join(source_folder, 'train_sat_re20.npy')\n",
        "train_sat_dataset = np.load(train_sat_name)\n",
        "\n",
        "# 学習用データ、検証用データを抽出する\n",
        "train_sat = train_sat_dataset[:365]\n",
        "val_sat   = train_sat_dataset[365:]\n",
        "\n",
        "# データの形状確認（例: (50, 24, 33, 25)）\n",
        "print(\"データ形状:\", train_sat.shape)\n",
        "\n",
        "flattened = train_sat.reshape(-1, train_sat.shape[2], train_sat.shape[3])\n",
        "images_to_show = flattened[:20]  # 最初の20枚\n",
        "\n",
        "# --- 画像を2×10で並べて表示 ---\n",
        "fig, axes = plt.subplots(2,10, figsize=(15, 15))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(images_to_show[i], cmap='gray', vmin=0, vmax=255)\n",
        "    ax.set_title(f\"#{i}\")\n",
        "    ax.axis('off')  # 枠線・目盛りを非表示\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9LdWYsqhXf0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}