{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakkunn7422/public_colab/blob/main/ai_contest_thema2_r8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yV094ubTpxU",
        "outputId": "cdc98cf8-4e85-49a8-fe9d-5dd1bcad2cef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Signateから 学習・評価用のデータ一式をダウンロード\n",
        "\n",
        "# 1. SIGNATE(https://signate.jp/) でアカウント登録\n",
        "# 初めてアクセスする際は、「コンペティションへの同意」を聞かれるので y 入力 必要\n",
        "\n",
        "!pip install signate\n",
        "!signate token --email=yakkunn7422@gmail.com --password=signate\n",
        "#!signate list\n",
        "!signate files --competition-id=270\n",
        "\n",
        "# 応募用\n",
        "!signate download --competition-id=270 --file-id=758 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=759 --path=/content/sample_data/\n",
        "\n",
        "# 衛星画像\n",
        "!signate download --competition-id=270 --file-id=755 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=756 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=757 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=762 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=763 --path=/content/sample_data/\n",
        "\n",
        "#気象情報\n",
        "\n",
        "!signate download --competition-id=270 --file-id=751 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=752 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=753 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=754 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=764 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=765 --path=/content/sample_data/\n",
        "!signate download --competition-id=270 --file-id=789 --path=/content/sample_data/\n",
        "\n"
      ],
      "metadata": {
        "id": "fLvnZzsiUt99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49980832-9412-45e5-860e-fb79e48d4bf0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting signate\n",
            "  Downloading signate-0.9.10-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from signate) (8.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from signate) (0.9.0)\n",
            "Collecting wget (from signate)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: urllib3>=1.26.7 in /usr/local/lib/python3.11/dist-packages (from signate) (2.4.0)\n",
            "Requirement already satisfied: six>=1.16 in /usr/local/lib/python3.11/dist-packages (from signate) (1.17.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from signate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from signate) (2.9.0.post0)\n",
            "Downloading signate-0.9.10-py3-none-any.whl (37 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=7976314eeebb70621cc16e006d52dc3207b644d14751eaeb9714fea9a9e46b02\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built wget\n",
            "Installing collected packages: wget, signate\n",
            "Successfully installed signate-0.9.10 wget-3.2\n",
            "\u001b[32mThe API Token has been downloaded successfully.\u001b[0m\n",
            "  fileId  name                   title                         size  updated_at\n",
            "--------  ---------------------  ----------------------  ----------  -------------------\n",
            "     757  sat_image_2018.zip     衛星画像2018年          1196800853  2019-09-10 02:36:11\n",
            "     755  sat_image_2017_01.zip  衛星画像2017年前半      1100993010  2019-09-10 02:36:11\n",
            "     756  sat_image_2017_02.zip  衛星画像2017年後半      1109747198  2019-09-10 02:36:11\n",
            "     762  sat_image_2016_01.zip  衛星画像2016年前半      1111602835  2019-09-10 02:36:11\n",
            "     763  sat_image_2016_02.zip  衛星画像2016年後半      1119587373  2019-09-10 02:36:11\n",
            "     753  met_data_2018_01.zip   気象データ2018年前半    1109284006  2019-09-10 02:36:11\n",
            "     754  met_data_2018_02.zip   気象データ2018年後半    1198905910  2019-09-10 02:36:11\n",
            "     751  met_data_2017_01.zip   気象データ2017年前半    2028546707  2019-09-10 02:36:11\n",
            "     752  met_data_2017_02.zip   気象データ2017年後半    2052181433  2019-09-10 02:36:11\n",
            "     764  met_data_2016_01.zip   気象データ2016年前半    2037719833  2019-09-10 02:36:11\n",
            "     765  met_data_2016_02.zip   気象データ2016年後半    2058580509  2019-09-10 02:36:11\n",
            "     789  add_met_data_2016.zip  気象データ2016年追加        120157  2019-09-10 02:36:11\n",
            "     758  inference_terms.csv    評価期間の詳細日時情報        6425  2019-09-10 02:20:11\n",
            "     759  sample_submit.csv      応募用サンプルファイル   102412552  2019-09-10 02:20:11\n",
            "inference_terms.csv\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sample_submit.csv\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2017_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2017_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2018.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2016_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "sat_image_2016_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2017_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2017_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2018_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2018_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2016_01.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "met_data_2016_02.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n",
            "add_met_data_2016.zip\n",
            "\n",
            "\u001b[32m\n",
            "Download completed.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nrYQAWjCnzEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# zipファイルをすべて解凍する。\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "source_folder      = '/content/sample_data/'  # zipファイルのおいている場所\n",
        "destination_folder = '/content/sample_data/'  # 解凍先\n",
        "\n",
        "# 衛星画像、気象データ対象ファイルのリスト（ファイル名だけ）\n",
        "zip_files = ['sat_image_2017_01.zip', 'sat_image_2017_02.zip', 'sat_image_2016_01.zip','sat_image_2016_02.zip',\n",
        "             'met_data_2017_01.zip','met_data_2017_02.zip','met_data_2016_01.zip','met_data_2016_02.zip','add_met_data_2016.zip',\n",
        "             'sat_image_2018.zip','met_data_2018_01.zip','met_data_2018_02.zip'\n",
        "             ]\n",
        "\n",
        "# zipファイルを解凍する関数\n",
        "def unzip_files(file_list, source_dir, extract_to):\n",
        "    os.makedirs(extract_to, exist_ok=True)  # 解凍先フォルダがなければ作る\n",
        "\n",
        "    for file_name in file_list:\n",
        "        zip_path = os.path.join(source_dir, file_name)  # YYYとファイル名を結合\n",
        "        if os.path.exists(zip_path) and zipfile.is_zipfile(zip_path):\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(extract_to)\n",
        "                print(f\"解凍しました: {zip_path} → {extract_to}\")\n",
        "        else:\n",
        "            print(f\"ファイルが存在しません。スキップします。: {zip_path}\")\n",
        "\n",
        "# 衛星画像、気象データ対象ファイル を解凍\n",
        "unzip_files(zip_files, source_folder, destination_folder)\n",
        "\n",
        "# 追加のデータを所定のフォルダにCOPY\n",
        "path    = os.path.join(destination_folder, 'data')\n",
        "if os.path.isdir(path):\n",
        "    shutil.copy2(\"/content/sample_data/data/HGT.300.3.2016010718.gz\", \"/content/sample_data/train/met/2016/01/07/\")\n",
        "    shutil.copy2(\"/content/sample_data/data/RH.1p5m.3.2016011218.gz\", \"/content/sample_data/train/met/2016/01/12/\")\n",
        "    shutil.copy2(\"/content/sample_data/data/TMP.850.3.2016011209.gz\", \"/content/sample_data/train/met/2016/01/12/\")\n",
        "    print('ファイルCOPY END')"
      ],
      "metadata": {
        "id": "oCOzT71KHNHX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65eb8b04-3a03-46b6-8bfb-112a6eef80ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "解凍しました: /content/sample_data/sat_image_2017_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2017_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2016_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2016_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2017_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2017_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2016_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2016_02.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/add_met_data_2016.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/sat_image_2018.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2018_01.zip → /content/sample_data/\n",
            "解凍しました: /content/sample_data/met_data_2018_02.zip → /content/sample_data/\n",
            "ファイルCOPY END\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# データセットの作成\n",
        "# ★24h画像 → 24h画像 推測を行う場合は ★箇所を有効に。\n",
        "#   学習用衛星画像データ　  train_sat.npy   (730,24,33,25,1)\n",
        "#   テスト用衛星画像データ  test_sat.npy    (50,24,33,25,1)\n",
        "#   学習用気象データ        train_met.npy   (730,24,33,25,34)\n",
        "#   テスト用気象データ      test_met.npy    (50,24,33,25,34)\n",
        "\n",
        "# 精度を上げるための施策\n",
        "# ・24h(1日)ではなく、96hの画像をもとに分析する\n",
        "# ・元の画像サイズを大きくする（初期は20分の1にResize）\n",
        "# ・不足している画像データの補完方法を見直し（初期は存在する前の時間をCOPY）\n",
        "\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "import cv2\n",
        "import gzip\n",
        "import shutil\n",
        "from datetime import datetime as dt\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 画像の読み込み時点で評価提出時のサイズにトリミングするフラグ\n",
        "size_custom_flg = False\n",
        "# 画像、データの縮小サイズ .通常サイズをreduction_size分の1に縮小する\n",
        "reduction_size = 20\n",
        "# 96時間分のデータでモデル作成時はTRUE\n",
        "date96_flag     = True\n",
        "\n",
        "source_folder       = '/content/sample_data/'\n",
        "destination_folder  = '/content/drive/MyDrive/2024_AIContest2/Thema2/'\n",
        "\n",
        "file_name = 'inference_terms.csv'\n",
        "csv_file_name = os.path.join(source_folder, file_name)\n",
        "\n",
        "'''\n",
        "# 風関連、上空200h,300h は除外 12個\n",
        "met_data_type_list = ['HGT.500', 'HGT.700', 'HGT.850',\n",
        "                     'PRMSL.msl',\n",
        "                     'RH.1p5m', 'RH.500', 'RH.700', 'RH.850',\n",
        "                     'TMP.1p5m', 'TMP.500', 'TMP.700', 'TMP.850']\n",
        "'''\n",
        "#SHAP分析で選択した11個\n",
        "met_data_type_list = ['HGT.500', 'HGT.700',\n",
        "                     'RH.300', 'RH.500',\n",
        "                     'TMP.1p5m', 'TMP.200', 'TMP.850',\n",
        "                     'UGRD.200',\n",
        "                     'VVEL.200', 'VVEL.300', 'VVEL.500']\n",
        "met_ch_num = len(met_data_type_list)\n",
        "'''\n",
        "# ALL\n",
        "met_data_type_list = ['HGT.200', 'HGT.300', 'HGT.500', 'HGT.700', 'HGT.850',\n",
        "                     'PRMSL.msl',\n",
        "                     'RH.1p5m', 'RH.300', 'RH.500', 'RH.700', 'RH.850',\n",
        "                     'TMP.1p5m', 'TMP.200', 'TMP.300', 'TMP.500', 'TMP.700', 'TMP.850',\n",
        "                     'UGRD.10m','UGRD.200', 'UGRD.300', 'UGRD.500', 'UGRD.700', 'UGRD.850',\n",
        "                     'VGRD.10m', 'VGRD.200', 'VGRD.300', 'VGRD.500', 'VGRD.700', 'VGRD.850',\n",
        "                     'VVEL.200', 'VVEL.300', 'VVEL.500', 'VVEL.700', 'VVEL.850']\n",
        "'''\n",
        "\n",
        "org_sat_size_h, org_sat_size_w = 672, 512 # 5キロメッシュ\n",
        "org_met_size_h, org_met_size_w = 168, 128 # 20キロメッシュ\n",
        "\n",
        "cus_sat_size_h, cus_sat_size_w = 420, 344 # 5キロメッシュ\n",
        "cus_met_size_h, cus_met_size_w =  42,  32 # 20キロメッシュ\n",
        "trim_y1, trim_y2 =  40,460\n",
        "trim_x1, trim_x2 = 128,472 # 本来は130,470だけど、metに合わせるために4の倍数\n",
        "\n",
        "# カスタムフラグ（トリミング）有無によって 元画像のサイズを設定\n",
        "#   ※気象情報のデータセット作成時もデータセットのサイズをそろえるために\n",
        "#     衛星画像のサイズを使用する。\n",
        "if size_custom_flg :\n",
        "    size_y, size_x = cus_sat_size_h, cus_sat_size_w\n",
        "else:\n",
        "    size_y, size_x = org_sat_size_h, org_sat_size_w\n",
        "\n",
        "# テスト対象期間の日付を取得する関数\n",
        "#\n",
        "def get_test_days_list():\n",
        "    # test_terms.csvからtest対象期間の日付を取得\n",
        "    test_terms = pd.read_csv(csv_file_name)\n",
        "\n",
        "    # OpenData_startカラムのデータを日付型のデータとして取得する\n",
        "    start_dates = pd.to_datetime(test_terms['OpenData_96hr_Start'])\n",
        "\n",
        "    # 96時間分のデータをすべて使用する場合、\n",
        "    if date96_flag :\n",
        "        days_list = []\n",
        "        for base_dt in start_dates:\n",
        "            for i in range(4):\n",
        "                days = base_dt + pd.Timedelta(days=i)\n",
        "                days_list.append(days)\n",
        "        days_list = [d.to_pydatetime() for d in days_list]\n",
        "    else:\n",
        "        # ★ 過去24時間の画像から24時間分の画像を推測するとき用\n",
        "        days_list = start_dates + pd.Timedelta(days=3)\n",
        "\n",
        "\n",
        "    return days_list\n",
        "\n",
        "# テスト対象期間の日付を取得\n",
        "print(\"--- get_test_days_list sample---\")\n",
        "sample_test_days_list = get_test_days_list()\n",
        "print(sample_test_days_list[:5])\n",
        "\n",
        "# 学習対象期間の日付を取得する関数\n",
        "def get_train_days_list():\n",
        "\n",
        "    start_date = dt(2016, 1, 1, 1)\n",
        "\n",
        "    # 日付を格納するリストを初期化する\n",
        "    days_list = []\n",
        "\n",
        "    # 学習データは365日×２年分与えられるため、合計730日分の日時を取得する\n",
        "    for i in range(2*365):\n",
        "\n",
        "        # start_dateから、i日後の日時を取得し、dateに代入する\n",
        "        date = start_date + timedelta(days=i)\n",
        "\n",
        "        # days_listにdateを追加する\n",
        "        days_list.append(date)\n",
        "\n",
        "    return days_list\n",
        "\n",
        "# 関数を実行して、学習データが提供されている日の日付を取得する\n",
        "#print(\"--- get_train_days_list sample---\")\n",
        "#sample_train_days_list = get_train_days_list()\n",
        "#print(sample_train_days_list[:5])\n",
        "\n",
        "# dataの日付情報からフルパスのファイル名を返す\n",
        "def create_file_name(date):\n",
        "\n",
        "    # 日付(date)から、年(year)、月(month)、日(day)、時(hour)を数値データとして取得する\n",
        "    year  = date.year\n",
        "    month = date.month\n",
        "    day   = date.day\n",
        "    hour  = date.hour\n",
        "\n",
        "    # 2018年ならばテストデータ(test)\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    # 2016年、2017年ならば学習データ(train)\n",
        "    else:\n",
        "        phase = 'train'\n",
        "\n",
        "    # ファイル名を指定する\n",
        "    file_name = f\"{phase}/sat/{year}-{month:02}-{day:02}/{year}-{month:02}-{day:02}-{hour:02}-00.fv.png\"\n",
        "\n",
        "    path_file_name = os.path.join(source_folder, file_name)\n",
        "\n",
        "    return path_file_name\n",
        "\n",
        "def get_sat_data(date):\n",
        "\n",
        "    # 衛星画像はファイルが存在しない時間(日)があるので、\n",
        "    # ない場合は前の時間の画像を使う。\n",
        "    file_exists = False\n",
        "    while not file_exists:\n",
        "        full_file_name = create_file_name(date)\n",
        "        file_exists = os.path.exists(full_file_name)\n",
        "        if not file_exists:\n",
        "            date = date - timedelta(hours=1)\n",
        "            print(f\"Warning: File not found: {full_file_name}\")\n",
        "            print(f\"Retry with date: {date}\")\n",
        "\n",
        "    # 画像の読み込み\n",
        "    sat_data = cv2.imread(full_file_name, 0)\n",
        "\n",
        "    # カスタムフラグ（トリミング）TRUE時は カスタムサイズでトリミング\n",
        "    if size_custom_flg :\n",
        "        img_resize = sat_data[trim_y1:trim_y2, trim_x1:trim_x2]\n",
        "        return img_resize\n",
        "    # トリミングなし\n",
        "    else:\n",
        "        return sat_data\n",
        "\n",
        "# 「2016年1月1日16時」時点の衛星画像を読み込む\n",
        "#print(\"--- get_sat_data sample---\")\n",
        "#sat_data = get_sat_data(dt(2016,1,1,1))\n",
        "#print(sat_data.shape)\n",
        "#print(sat_data)\n",
        "\n",
        "# 気象データのgzを読み込んで配列で返す\n",
        "def Read_gz_Binary(file_path):\n",
        "\n",
        "    # 1. 変数file_tmpに「元のファイル名_tmp」となる文字列を代入\n",
        "    file_tmp = file_path + \"_tmp\"\n",
        "\n",
        "    # 2. gzipファイルを、「読み込み(r)モード & バイナリ(b)モード」 で開き、\n",
        "    #     開いたファイルオブジェクトをf_inに代入する\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "\n",
        "        # 3. ファイル名を「元のファイル名_tmp」としたファイルを\n",
        "        #     open()関数の「書き込み(w)モード & バイナリ(b)モード」で新規作成し、\n",
        "        #     ファイルオブジェクトをf_outとする\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "\n",
        "            # 4. f_inのファイルオブジェクトをf_outにコピーする\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    # 5. バイナリファイルをNumPy配列として読み込む\n",
        "    met_data = np.fromfile(file_tmp, np.float32)\n",
        "\n",
        "    # 6. 一時的に作成した展開済みのバイナリファイル「元のファイル名_tmp」を削除する\n",
        "    os.remove(file_tmp)\n",
        "\n",
        "    # 7. 読み込んだ配列の形状をOriginaの気象データサイズに整形する\n",
        "    #    1次元の配列を2次元に整形\n",
        "    met_data = met_data.reshape( [org_met_size_h, org_met_size_w] )\n",
        "\n",
        "    # カスタムフラグ（トリミング）TRUE時は カスタムサイズでトリミング\n",
        "    if size_custom_flg :\n",
        "#        met_resize = met_data[ (trim_y1/4):(trim_y2/4), (trim_x1/4):(trim_x2/4)]\n",
        "        met_resize = met_data[ 10:115, 32:118]\n",
        "        return met_resize\n",
        "    # トリミングなし\n",
        "    else:\n",
        "        return met_data\n",
        "\n",
        "# HGT.200.3.2016010100.gz の気象データを確認\n",
        "#print(\"--- Read_gz_Binary sample---\")\n",
        "#tmp_file_name = '/content/sample_data/train/met/2016/01/01/HGT.200.3.2016010100.gz'\n",
        "#tmp_array = Read_gz_Binary(tmp_file_name)\n",
        "#print(tmp_array.shape)\n",
        "#print(tmp_array)\n",
        "\n",
        "## 気象データの欠損部分を補完する。\n",
        "def fill_lack_data(data):\n",
        "\n",
        "    # カスタムフラグ（トリミング）FALSE時は 補完を実施\n",
        "    if not size_custom_flg  :\n",
        "        ## 1. 北側、南側の未計測部分を補間する(上下)\n",
        "        # 北側の未計測部分を補間する\n",
        "        data[0:2] = data[2]\n",
        "        # 南側の未計測部分を補間する\n",
        "        data[154:] = data[153]\n",
        "\n",
        "        ## 2. 西側の未計測部分を補間する(左右)\n",
        "\n",
        "        # 西側の未計測部分を補間する\n",
        "        data[:, :8] = data[:,8].reshape(-1, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 気象データを読み込むライブラリ\n",
        "def get_met_data(date, data_type):\n",
        "\n",
        "    # 日付(date)から、年(year)、月(month)、日(day)、時(hour)を数値データとして取得する\n",
        "    year  = date.year\n",
        "    month = date.month\n",
        "    day  = date.day\n",
        "    hour = date.hour\n",
        "\n",
        "    # 2018年ならばテストデータ(test)\n",
        "    if year == 2018:\n",
        "        phase = 'test'\n",
        "    # 2016年、2017年ならば学習データ(train)\n",
        "    else:\n",
        "        phase = 'train'\n",
        "\n",
        "    # ファイル名を指定する\n",
        "    file_name = f'{phase}/met/{year}/{month:02}/{day:02}/{data_type}.3.{year}{month:02}{day:02}{hour:02}.gz'\n",
        "\n",
        "    full_file_name = os.path.join(source_folder, file_name)\n",
        "\n",
        "    # データの読み込み(Read_gz_Binary) -> 空白箇所の穴埋め(fill_lack_data)の順番で処理を行う\n",
        "    met_data = fill_lack_data(Read_gz_Binary(full_file_name))\n",
        "\n",
        "    return met_data\n",
        "\n",
        "# 衛星画像のデータセットを作成\n",
        "def make_sat_dataset(resize, phase):\n",
        "\n",
        "    # 引数phaseの値に従って、\n",
        "    # 学習用、もしくはテスト用のデータが与えられている日付を取得する\n",
        "    ## [get_train_days_list()、get_test_days_list()はともにタスク1で作成した関数]\n",
        "    if phase == 'train':\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    # データセットを格納するリストの初期化\n",
        "    dataset = []\n",
        "\n",
        "    # start_date_listから、順番にデータが与えられている日付を取得し、\n",
        "    # 変数start_dateに代入する\n",
        "    for start_date in start_date_list:\n",
        "\n",
        "        # 一日分の衛星画像データを格納するリストの初期化\n",
        "        data_in_1day = []\n",
        "\n",
        "        # 24時間分の衛星画像を順番に取得する\n",
        "        for i in range(24):\n",
        "            # start_dateから、時間を「i時間」分進める\n",
        "            date = start_date + timedelta(hours=i)\n",
        "\n",
        "            # 与えられた日時(date)における衛星画像データを読み込む\n",
        "            ## [get_sat_data()関数は当タスク内で作成した関数]\n",
        "            sat_data = get_sat_data(date)\n",
        "\n",
        "            # 引数resizeの値に従って、データを縮小する\n",
        "            #   cv2.resize(入力画像, dsize(横,縦),interpolation)\n",
        "            resized_data = cv2.resize(sat_data, ( int(size_x / resize), int(size_y / resize) ),\n",
        "                                                          interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # data_in_1dayリストへの値の追加\n",
        "            data_in_1day.append(resized_data)\n",
        "\n",
        "        # 1日分(24時間分)のデータを格納したリストをNumPy配列に変換する\n",
        "        # 形状は(24, 高さ, 幅, 1)\n",
        "        data_in_1day = np.array(data_in_1day, dtype='uint8').reshape(24, int(size_y / resize), int(size_x / resize), 1)\n",
        "\n",
        "        # 取得した1日分のNumPy配列をdatasetリストに追加する\n",
        "        dataset.append(data_in_1day)\n",
        "\n",
        "    # 対象期間に含まれる全データを格納したリストをNumPy配列に変換する\n",
        "    # 形状は(対象期間の日数, 24, 高さ, 幅, 1)\n",
        "    dataset = np.array(dataset, dtype='uint8').reshape(len(start_date_list), 24, int(size_y / resize), int(size_x / resize), 1)\n",
        "\n",
        "    # 保存するnpyファイル名を指定する\n",
        "    if date96_flag :\n",
        "        save_name = f'{phase}_sat_{size_custom_flg}_{resize}_96.npy'\n",
        "    else:\n",
        "        save_name = f'{phase}_sat_{size_custom_flg}_{resize}.npy'\n",
        "\n",
        "    full_save_name = os.path.join(destination_folder, save_name)\n",
        "\n",
        "    # NumPy配列をnpyファイルとして保存する\n",
        "    np.save(full_save_name, dataset)\n",
        "\n",
        "# 気象データのデータセットを作成\n",
        "def make_met_dataset(resize, phase, data_type_list):\n",
        "\n",
        "    if phase == 'train':\n",
        "        start_date_list = get_train_days_list()\n",
        "    elif phase == 'test':\n",
        "        start_date_list = get_test_days_list()\n",
        "\n",
        "    # 全種類、全日時分のデータを格納するリストの初期化\n",
        "    dataset = []\n",
        "\n",
        "    for start_date in start_date_list:\n",
        "\n",
        "        #print(start_date)\n",
        "        # 引数data_type_listから、\n",
        "        # 気象データの種類を示す文字列を1つずつ取り出す。\n",
        "        for count, data_type in enumerate(data_type_list):\n",
        "\n",
        "            # 1種類、1日分のデータを格納するリストの初期化\n",
        "            one_type_in_1day = []\n",
        "\n",
        "            for i in range(24):\n",
        "\n",
        "                date = start_date + timedelta(hours=i)\n",
        "                hour = date.hour\n",
        "\n",
        "                \"\"\"データ補間処理の方針\n",
        "                A:\n",
        "                3で割り切ることのできる時刻T(0時, 3時, 6時, 9時, 12時, 15時, 18時, 21時)は、\n",
        "                気象データが提供されているため、単純にデータを読み込む。\n",
        "\n",
        "                B:\n",
        "                それ以外の時刻T+1とT+2については、\n",
        "                時刻Tと時刻T+3時点の気象データを利用して、値を補間する。\n",
        "\n",
        "                \"\"\"\n",
        "\n",
        "                # 時刻がT[0, 3, 6, 9, 12, 15, 18, 21]時の場合\n",
        "                if hour % 3 == 0:\n",
        "\n",
        "                    met_data = get_met_data(date, data_type)\n",
        "\n",
        "                # 時刻がT+1[1, 4, 7, 10, 13, 16, 19, 22]時の場合\n",
        "                elif hour % 3 == 1:\n",
        "\n",
        "                    # １時間前(時刻T)の気象データを読み込む\n",
        "                    before_1h = date - timedelta(hours=1)\n",
        "                    data_before_1h = get_met_data(before_1h, data_type)\n",
        "\n",
        "                    # ２時間後(時刻T+3)の気象データを読み込む\n",
        "                    after_2h = date + timedelta(hours=2)\n",
        "                    data_after_2h = get_met_data(after_2h, data_type)\n",
        "\n",
        "                    # 前後の値を利用して、補間する\n",
        "                    met_data = (2/3) * data_before_1h + (1/3) * data_after_2h\n",
        "\n",
        "                # 時刻がT+2[2, 5, 8, 11, 14, 17, 20, 23]時の場合\n",
        "                else:\n",
        "\n",
        "                    # ２時間前(時刻T)の気象データを読み込む\n",
        "                    before_2h = date - timedelta(hours=2)\n",
        "                    data_before_2h = get_met_data(before_2h, data_type)\n",
        "\n",
        "                    # １時間後(時刻T+3)の気象データを読み込む\n",
        "                    after_1h = date + timedelta(hours=1)\n",
        "                    data_after_1h = get_met_data(after_1h, data_type)\n",
        "\n",
        "                    # 前後の値を利用して、補間する\n",
        "                    met_data = (1/3) * data_before_2h + (2/3) * data_after_1h\n",
        "\n",
        "                # 引数resizeの値に従って、データを縮小する\n",
        "                resized_data = cv2.resize(met_data, (int(size_x / resize), int(size_y / resize)),\n",
        "                                                              interpolation=cv2.INTER_AREA)\n",
        "\n",
        "                # 1種類、1時刻分のデータをone_type_in_1dayリストへ追加する\n",
        "                one_type_in_1day.append(resized_data)\n",
        "\n",
        "            one_type_in_1day = np.array(one_type_in_1day).reshape(24, int(size_y / resize), int(size_x / resize), 1)\n",
        "\n",
        "            # 1種類目はall_type_in_1dayに代入する\n",
        "            if count == 0:\n",
        "                all_type_in_1day = one_type_in_1day\n",
        "            # 2種類目以降はall_type_in_1dayにチャンネルの次元で結合する\n",
        "            else:\n",
        "                all_type_in_1day = np.concatenate([all_type_in_1day, one_type_in_1day], axis=3)\n",
        "\n",
        "        # 全種類、1日分のデータをdatasetリストへ追加する\n",
        "        dataset.append(all_type_in_1day)\n",
        "\n",
        "    dataset = np.array(dataset, dtype='float32').reshape(len(start_date_list), 24, int(size_y / resize), int(size_x / resize), len(data_type_list))\n",
        "\n",
        "    if date96_flag :\n",
        "        save_name = f'{phase}_met{met_ch_num}_{size_custom_flg}_{resize}_96.npy'\n",
        "    else:\n",
        "        save_name = f'{phase}_met{met_ch_num}_{size_custom_flg}_{resize}.npy'\n",
        "\n",
        "    full_save_name = os.path.join(destination_folder, save_name)\n",
        "    np.save(full_save_name, dataset)\n",
        "\n",
        "#print ('---- make_sat_dataset train ----')\n",
        "#make_sat_dataset(resize=reduction_size, phase='train')\n",
        "print ('---- make_sat_dataset test ----')\n",
        "make_sat_dataset(resize=reduction_size, phase='test')\n",
        "#print ('---- make_met_dataset train ----')\n",
        "#make_met_dataset(resize=reduction_size, phase='train', data_type_list=met_data_type_list)\n",
        "print ('---- make_met_dataset test ----')\n",
        "make_met_dataset(resize=reduction_size, phase='test', data_type_list=met_data_type_list)"
      ],
      "metadata": {
        "id": "SJF3U1g6isHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c04d674c-f27a-4cbf-e4d4-efc6e06b8348"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- get_test_days_list sample---\n",
            "[datetime.datetime(2018, 1, 2, 16, 0), datetime.datetime(2018, 1, 3, 16, 0), datetime.datetime(2018, 1, 4, 16, 0), datetime.datetime(2018, 1, 5, 16, 0), datetime.datetime(2018, 1, 9, 16, 0)]\n",
            "---- make_sat_dataset test ----\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-02-01/2018-02-01-21-00.fv.png\n",
            "Retry with date: 2018-02-01 20:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-02-02/2018-02-02-09-00.fv.png\n",
            "Retry with date: 2018-02-02 08:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-02-07/2018-02-07-21-00.fv.png\n",
            "Retry with date: 2018-02-07 20:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-02-15/2018-02-15-23-00.fv.png\n",
            "Retry with date: 2018-02-15 22:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-02-16/2018-02-16-11-00.fv.png\n",
            "Retry with date: 2018-02-16 10:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-02-22/2018-02-22-21-00.fv.png\n",
            "Retry with date: 2018-02-22 20:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-04-13/2018-04-13-11-00.fv.png\n",
            "Retry with date: 2018-04-13 10:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-07/2018-06-07-17-00.fv.png\n",
            "Retry with date: 2018-06-07 16:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-08/2018-06-08-05-00.fv.png\n",
            "Retry with date: 2018-06-08 04:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-15/2018-06-15-21-00.fv.png\n",
            "Retry with date: 2018-06-15 20:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-21/2018-06-21-16-00.fv.png\n",
            "Retry with date: 2018-06-21 15:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-22/2018-06-22-04-00.fv.png\n",
            "Retry with date: 2018-06-22 03:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-06-30/2018-06-30-09-00.fv.png\n",
            "Retry with date: 2018-06-30 08:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-07-19/2018-07-19-07-00.fv.png\n",
            "Retry with date: 2018-07-19 06:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-07-19/2018-07-19-19-00.fv.png\n",
            "Retry with date: 2018-07-19 18:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-10-11/2018-10-11-22-00.fv.png\n",
            "Retry with date: 2018-10-11 21:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-10-12/2018-10-12-10-00.fv.png\n",
            "Retry with date: 2018-10-12 09:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-11-22/2018-11-22-20-00.fv.png\n",
            "Retry with date: 2018-11-22 19:00:00\n",
            "Warning: File not found: /content/sample_data/test/sat/2018-11-23/2018-11-23-08-00.fv.png\n",
            "Retry with date: 2018-11-23 07:00:00\n",
            "---- make_met_dataset test ----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "id": "3RVAIY03KDMU",
        "outputId": "03bc44bd-f9df-46b4-8ee9-9c2afbb3e3a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデリング (train) / 評価 (test)\n",
        "\n",
        "# 改善案)\n",
        "# ・正規化の時にfloat64で正規化\n",
        "# ・クロスバリデーション実施\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ConvLSTM2D,TimeDistributed\n",
        "from tensorflow.keras.layers import Conv2D,Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/2024_AIContest2/Thema2/'\n",
        "\n",
        "train_sat_name = os.path.join(source_folder, 'train_sat_False_20.npy')\n",
        "test_sat_name  = os.path.join(source_folder,  'test_sat_False_20_96.npy')\n",
        "train_met_name = os.path.join(source_folder, 'train_met11_False_20.npy')\n",
        "test_met_name  = os.path.join(source_folder,  'test_met11_False_20_96.npy')\n",
        "best_h5        = 'my_model_Epoch139_loss0.0799_valloss0.0820.weights.h5'\n",
        "best_h5_name   = os.path.join(source_folder, best_h5)\n",
        "\n",
        "mode            = 'train' # 学習時は「train」評価は「test」\n",
        "size_custom_flg = False   # 画像の読み込み時点で評価提出時のサイズにトリミングするフラグ\n",
        "merge_flag      = True    # True時、気象データを使用する\n",
        "reduction_size  = 15      # 画像、気象データの縮小サイズ 通常サイズをreduction_size分の1に縮小する\n",
        "filter_size     = 20      # モデルパラメータ フィルタサイズ\n",
        "met_ch_size     = 11      # met:12\n",
        "data_96flag     = False   # 前4日分のデータを使って評価する場合はTrue\n",
        "batch_size      = 16\n",
        "num_of_epoch    = 2\n",
        "\n",
        "\n",
        "### サイズの定義  ###\n",
        "org_sat_size_h, org_sat_size_w = 672, 512 # 5キロメッシュ\n",
        "org_met_size_h, org_met_size_w = 168, 128 # 20キロメッシュ\n",
        "\n",
        "cus_sat_size_h, cus_sat_size_w = 420, 344 # 5キロメッシュ\n",
        "cus_met_size_h, cus_met_size_w =  42,  32 # 20キロメッシュ\n",
        "\n",
        "if size_custom_flg :\n",
        "    size_y, size_x = cus_sat_size_h, cus_sat_size_w\n",
        "else:\n",
        "    size_y, size_x = org_sat_size_h, org_sat_size_w\n",
        "\n",
        "# EncoderのINPUTデータのチャネルサイズ\n",
        "# 気象データありの場合は 衛星画像CH(1)＋気象データCHとなる。\n",
        "if merge_flag :\n",
        "    input_ch_size = 1 + met_ch_size\n",
        "else:\n",
        "    input_ch_size = 1\n",
        "\n",
        "\n",
        "# ConvLSTMに渡すINPUT Shapeの時間\n",
        "if date96_flag:\n",
        "    input_hours = 96\n",
        "else:\n",
        "    input_hours = 24\n",
        "\n",
        "###------ モデルの作成 --------###\n",
        "# モデルを初期化する\n",
        "model = Sequential()\n",
        "\n",
        "# Encoder 追加\n",
        "# 改善POINT filters、kernel_size\n",
        "model.add(ConvLSTM2D(\n",
        "  filters=filter_size, kernel_size=(3, 3),\n",
        "  padding='same', return_sequences=False,\n",
        "  activation='tanh', recurrent_activation='sigmoid',\n",
        "  input_shape=(input_hours, int(size_y / reduction_size), int(size_x / reduction_size),input_ch_size)))\n",
        "\n",
        "# EncoderとDecoderをつなぐ中間部分 追加\n",
        "def repeat_last_status(x):\n",
        "    x = tf.reshape(x, (-1, 1, int(size_y / reduction_size), int(size_x / reduction_size), filter_size))\n",
        "    copied_x = tf.identity(x)\n",
        "    for _ in range(23):\n",
        "        x = tf.concat([x, copied_x], axis=1)\n",
        "    return x\n",
        "model.add(Lambda(repeat_last_status))\n",
        "\n",
        "# Decoder 追加\n",
        "model.add(ConvLSTM2D(\n",
        "  filters=filter_size, kernel_size=(3, 3),\n",
        "  padding='same', return_sequences=True,\n",
        "  activation='tanh', recurrent_activation='sigmoid'))\n",
        "\n",
        "# 出力層 追加\n",
        "model.add(TimeDistributed(Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')))\n",
        "\n",
        "# モデルをコンパイルする\n",
        "model.compile(loss='mae', optimizer='adam')\n",
        "\n",
        "###------ モデルの作成 END --------###\n",
        "\n",
        "###------ 気象情報の正規化 関数定義 --------###\n",
        "def get_met_max_min(dataset):\n",
        "    # 気象データのチャンネル数を取得する 例) (730, 24, 33, 25, 12)\n",
        "    n_data_type = dataset.shape[4]\n",
        "\n",
        "    max_array = []\n",
        "    min_array = []\n",
        "\n",
        "    # チャネル数分のインデックスを順番に取得する\n",
        "    for i in range(n_data_type):\n",
        "\n",
        "        # チャンネルの次元をインデックスiで指定し、\n",
        "        # チャンネルごとの最大値、最小値を取得する\n",
        "        max_value = np.max(dataset[:,:,:,:,i])\n",
        "        min_value = np.min(dataset[:,:,:,:,i])\n",
        "\n",
        "        max_array.append(max_value)\n",
        "        min_array.append(min_value)\n",
        "\n",
        "    max_array = np.array(max_array)\n",
        "    min_array = np.array(min_array)\n",
        "\n",
        "    return max_array, min_array\n",
        "\n",
        "def normalize_met_data(met_data, max_array, min_array):\n",
        "\n",
        "    #\n",
        "    n_data_type = met_data.shape[4]\n",
        "\n",
        "    for i in range(n_data_type):\n",
        "        # max_array, min_arrayを利用して、正規化を実行する\n",
        "        met_data[:, :, :, :, i] = (met_data[:, :, :, :, i] - min_array[i]) / (max_array[i] - min_array[i])\n",
        "\n",
        "    return met_data\n",
        "\n",
        "###------ 気象情報の正規化 関数定義 END --------###\n",
        "\n",
        "###------ 学習／評価 本番処理 --------###\n",
        "if mode == 'train':\n",
        "    ### データ前処理 ###\n",
        "    # npyを読み込む\n",
        "    train_sat_dataset = np.load(train_sat_name)\n",
        "    print('-- 学習用 衛星画像 データセットサイズ --')\n",
        "    print(train_sat_dataset.shape)\n",
        "\n",
        "    print('衛星画像データの学習用データ、検証用データを抽出する')\n",
        "    # 学習用データ、検証用データを抽出する\n",
        "    train_sat_data = train_sat_dataset[:365]\n",
        "    val_sat_data   = train_sat_dataset[365:]\n",
        "\n",
        "    print('衛星画像データを正規化する')\n",
        "    # 衛星画像データを正規化する\n",
        "    train_sat_data = train_sat_data.astype(np.float32) / 255\n",
        "    val_sat_data   = val_sat_data.astype(np.float32)   / 255\n",
        "\n",
        "    if merge_flag :\n",
        "        train_met_dataset = np.load(train_met_name)\n",
        "        print('-- 学習用 気象データ データセットサイズ --')\n",
        "        print(train_met_dataset.shape)\n",
        "\n",
        "        print('気象データの学習用データ、検証用データを抽出する')\n",
        "        # 学習用データ、検証用データを抽出する\n",
        "        train_met_data = train_met_dataset[:365]\n",
        "        val_met_data   = train_met_dataset[365:]\n",
        "\n",
        "        # 最大・最小値の取得\n",
        "        max_array, min_array = get_met_max_min(dataset=train_met_data)\n",
        "\n",
        "        # 評価(test)時に使用するため、ファイルにSaveしておく。\n",
        "        max_save_name = f'max_array_{met_ch_size}ch.npy'\n",
        "        min_save_name = f'min_array_{met_ch_size}ch.npy'\n",
        "        met_max_fname = os.path.join(source_folder, max_save_name)\n",
        "        met_min_fname = os.path.join(source_folder, min_save_name)\n",
        "        np.save(met_max_fname, max_array)\n",
        "        np.save(met_min_fname, min_array)\n",
        "\n",
        "        # 正規化実施\n",
        "        print('気象データ trainを正規化する')\n",
        "        train_met_data = normalize_met_data(met_data=train_met_data,\n",
        "                                               max_array=max_array, min_array=min_array)\n",
        "\n",
        "        print('気象データ valを正規化する')\n",
        "        # スケールの基準はtrainと同じにしておく必要がある。\n",
        "        # モデルが本来知るべきでない未来の情報を学習に取り込むことになるため\n",
        "        val_met_data = normalize_met_data(met_data=val_met_data,\n",
        "                                               max_array=max_array, min_array=min_array)\n",
        "\n",
        "        # 衛星画像と気象情報をマージ\n",
        "        train_data = np.concatenate([train_sat_data, train_met_data], axis=4)\n",
        "        val_data   = np.concatenate([val_sat_data, val_met_data], axis=4)\n",
        "\n",
        "    else:\n",
        "        train_data = train_sat_data\n",
        "        val_data   = val_sat_data\n",
        "\n",
        "    print('-- 学習用 データセットサイズ --')\n",
        "    print(train_data.shape)\n",
        "    print(val_data.shape)\n",
        "\n",
        "    print('入力データと正解データを作る → 次の1日分が正解データ')\n",
        "    # 入力データと正解データを作る → 次の1日分が正解データ\n",
        "    if date96_flag:\n",
        "        X_index = np.arange(361)   # 0-360(361個) のIndex作成\n",
        "        Y_index = np.arange(4,365) # 4-364(361個) のIndex作成\n",
        "    else:\n",
        "        X_index = np.arange(364)   # 0-363(364個) のIndex作成\n",
        "        Y_index = np.arange(1,365) # 1-364(364個) のIndex作成\n",
        "\n",
        "    print('データ作成')\n",
        "    X_train = train_data[X_index]\n",
        "    Y_train = train_sat_data[Y_index] # 予測対象は衛星画像のみ\n",
        "    X_val   = val_data[X_index]\n",
        "    Y_val   = val_sat_data[Y_index]   # 予測対象は衛星画像のみ\n",
        "\n",
        "    print('モデルを使って学習実施')\n",
        "    ### モデルを使って学習実施 ###\n",
        "    # 検証用データをタプルにまとめる\n",
        "    validation_data = (X_val, Y_val)\n",
        "\n",
        "    # EarlyStoppingの設定\n",
        "    e_stopping = EarlyStopping(monitor='val_loss', patience=30)\n",
        "\n",
        "    # Checkpointの設定\n",
        "    checkpoint = ModelCheckpoint(filepath='my_model_Epoch{epoch}_loss{loss:.4f}_valloss{val_loss:.4f}.weights.h5',\n",
        "                                monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "\n",
        "    # callbacksをリストにまとめる\n",
        "    callbacks = [e_stopping, checkpoint]\n",
        "\n",
        "    # 学習を実行する\n",
        "    history = model.fit(x=X_train, y=Y_train, batch_size=batch_size, epochs=num_of_epoch,\n",
        "                    validation_data=validation_data, callbacks=callbacks)\n",
        "\n",
        "    # 結果をグラフで出力\n",
        "    # Plot training & validation loss values\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Model loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    ### データ前処理 ###\n",
        "    # npyを読み込む\n",
        "    test_sat_dataset = np.load(test_sat_name)\n",
        "    print('test_sat_dataset データサイズ')\n",
        "    print(test_sat_dataset.shape)\n",
        "\n",
        "    # 前処理 正規化\n",
        "    print('test_sat_dataset 正規化実施')\n",
        "    test_sat_dataset = test_sat_dataset.astype(np.float32) / 255\n",
        "\n",
        "    if merge_flag :\n",
        "        # 気象データをマージ\n",
        "        # npyを読み込む\n",
        "        test_met_dataset = np.load(test_met_name)\n",
        "        print('test_met_dataset データサイズ')\n",
        "        print(test_met_dataset.shape)\n",
        "\n",
        "        print('衛星画像データを正規化する')\n",
        "        # 最大・最小値の取得\n",
        "        max_save_name = f'max_array_{met_ch_size}ch.npy'\n",
        "        min_save_name = f'min_array_{met_ch_size}ch.npy'\n",
        "        met_max_fname = os.path.join(source_folder, max_save_name)\n",
        "        met_min_fname = os.path.join(source_folder, min_save_name)\n",
        "        max_array = np.load(met_max_fname)\n",
        "        min_array = np.load(met_min_fname)\n",
        "\n",
        "        # 正規化実施\n",
        "        test_met_dataset = normalize_met_data(met_data=test_met_dataset,\n",
        "                                               max_array=max_array, min_array=min_array)\n",
        "        # 衛星画像と気象情報をマージ\n",
        "        X_test = np.concatenate([test_sat_dataset, test_met_dataset], axis=4)\n",
        "\n",
        "    else:\n",
        "        X_test = test_sat_dataset\n",
        "\n",
        "    print('test_dataset サイズ')\n",
        "    print(X_test.shape)\n",
        "\n",
        "    model.load_weights(best_h5_name)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    Y_pred = Y_pred * 255.0\n",
        "    Y_pred = np.round(Y_pred)\n",
        "\n",
        "    #np.save('predicted_satellite.npy', Y_pred)\n",
        "\n",
        "    # 評価結果を出力\n",
        "    # 出力サイズの定義\n",
        "    # データセットのタイミングでトリミングあり\n",
        "    if size_custom_flg :\n",
        "        resized_h, resized_w = cus_sat_size_h, cus_sat_size_w # Resize前のサイズ\n",
        "        crop_y1, crop_y2     =  0, 420    # データセットのタイミングでトリミング済み\n",
        "        crop_x1, crop_x2     =  2, 342    # データセットのトリミングから、両端2グリッドずつトリミング\n",
        "    # トリミングなし\n",
        "    else:\n",
        "        resized_h, resized_w = org_sat_size_h, org_sat_size_w # Resize前のサイズ\n",
        "        crop_y1, crop_y2     =  40, 460   # 縦方向 切り出し\n",
        "        crop_x1, crop_x2     = 130, 470   # 横方向 切り出し\n",
        "\n",
        "    target_hours         = [5, 11, 17, 23]  # 時刻インデックス\n",
        "\n",
        "    # 各画像サイズ：420x340、200枚 → 全体サイズ：(420*200, 340)\n",
        "    cropped_height = crop_y2 - crop_y1  # 420\n",
        "    cropped_width  = crop_x2 - crop_x1  # 340\n",
        "    total_images   = 50 * 4             # 200枚\n",
        "\n",
        "    # 空の大きな配列（縦長）を作成\n",
        "    vertical_concat_image = np.zeros((total_images * cropped_height, cropped_width), dtype=np.uint8)\n",
        "\n",
        "    # 画像処理と縦方向への結合\n",
        "    row_pos = 0\n",
        "    for day in range(50):\n",
        "        for hour in target_hours:\n",
        "            img_small = Y_pred[day, hour, :, :, 0]  # (33, 25)\n",
        "            img_resized = cv2.resize(img_small, (resized_w, resized_h), interpolation=cv2.INTER_LINEAR)\n",
        "            cropped_img = img_resized[crop_y1:crop_y2, crop_x1:crop_x2]\n",
        "            cropped_img_uint8 = np.clip(cropped_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "            vertical_concat_image[row_pos:row_pos + cropped_height, :] = cropped_img_uint8\n",
        "            row_pos += cropped_height\n",
        "\n",
        "    # DataFrameに変換（Index付き）\n",
        "    df = pd.DataFrame(vertical_concat_image)\n",
        "    df.index = np.arange(vertical_concat_image.shape[0])  # 0〜83999\n",
        "\n",
        "    save_file_name = 'submit_image_with_index.csv'\n",
        "    save_file_path = os.path.join(source_folder, save_file_name)\n",
        "\n",
        "    # CSVとして保存（行番号付き）\n",
        "    df.to_csv(save_file_path, header=False, index=True)\n",
        "    print('-- submit_image_with_index.csv Saved!')\n",
        "\n"
      ],
      "metadata": {
        "id": "DMIpsh6PT8GR",
        "outputId": "c46c182c-cd1f-44a6-a8fd-5a0bd51a2df7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0f9a9d51673e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/distribute/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/__internal__/distribute/combinations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0menv\u001b[0m \u001b[0;31m# line: 456\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m \u001b[0;31m# line: 365\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombinations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0min_main_process\u001b[0m \u001b[0;31m# line: 418\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/combinations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_all_reduce_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulti_process_runner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_server_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcross_device_ops_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/cross_device_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvalue_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/values.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstruct_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpacked_distributed_variable\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpacked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreduce_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mag_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_ragged_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatching\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdense_to_sparse_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/service/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    417\u001b[0m \"\"\"\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_dataset_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_service_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregister_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_service_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompression_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_server_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pywrap_utils_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# ==============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_experimental_dataset_ops\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mged_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_variable_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplatform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_logging\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minternal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/ragged/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0mdocstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \"\"\"\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mragged\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/ragged/ragged_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2318\u001b[0m \u001b[0;31m# ===============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2319\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RaggedTensorSpec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2320\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mtype_spec_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.RaggedTensorSpec\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2321\u001b[0m class RaggedTensorSpec(\n\u001b[1;32m   2322\u001b[0m     type_spec.BatchableTypeSpec, internal_types.RaggedTensorSpec):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec_registry.py\u001b[0m in \u001b[0;36mdecorator_fn\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     57\u001b[0m                        (cls.__module__, cls.__name__, _TYPE_SPEC_TO_NAME[cls]))\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_NAME_TO_TYPE_SPEC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       raise ValueError(\"Name %s has already been registered for class %s.%s.\" %\n\u001b[0m\u001b[1;32m     60\u001b[0m                        (name, _NAME_TO_TYPE_SPEC[name].__module__,\n\u001b[1;32m     61\u001b[0m                         _NAME_TO_TYPE_SPEC[name].__name__))\n",
            "\u001b[0;31mValueError\u001b[0m: Name tf.RaggedTensorSpec has already been registered for class tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUTno\n",
        "# HGT: PMMSL:海面気温 RH:湿度 TMP:気温 UGRD:東西風 VGRD:南北風 VVEL:鉛直流\n",
        "\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import gzip\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "# 気象データを読み込む関数; Read_gz_Binary を実装する\n",
        "def Read_gz_Binary(file_path):\n",
        "    file_tmp = file_path + \"_tmp\"\n",
        "    with gzip.open(file_path, 'rb') as f_in:\n",
        "        with open(file_tmp, 'wb') as f_out:\n",
        "            shutil.copyfileobj(f_in, f_out)\n",
        "\n",
        "    # バイナリファイルをNumPy配列として読み込む\n",
        "    met_data = np.fromfile(file_tmp, np.float32)\n",
        "    os.remove(file_tmp)\n",
        "\n",
        "    # 配列の次元に変更を加える\n",
        "    met_data = met_data.reshape( [168,128] )\n",
        "\n",
        "    return met_data\n",
        "\n",
        "# 気象データのファイルパスを指定する\n",
        "file_path = '/content/sample_data/train/met/2016/01/02/TMP.1p5m.3.2016010218.gz'\n",
        "\n",
        "# 気象データを読み込む\n",
        "met_data = Read_gz_Binary(file_path)\n",
        "\n",
        "# 読み込んだ気象データに関する基本情報を出力する\n",
        "print('気象データに関する基本情報')\n",
        "print(type(met_data))\n",
        "print(met_data.shape)\n",
        "\n",
        "# 最大値の出力\n",
        "print('最大値: ', np.max(met_data))\n",
        "# 最小値の出力\n",
        "print('最小値: ', np.min(met_data))\n",
        "# 平均値の出力\n",
        "print('平均値: ', np.mean(met_data))\n",
        "\n",
        "#print(met_data)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(met_data, 'gray')\n",
        "\n",
        "# fill_lack_data()関数を実装する\n",
        "def fill_lack_data(data):\n",
        "    ## 1. 北側、南側の未計測部分を補間する(上下)\n",
        "    # 北側の未計測部分を補間する\n",
        "    data[0:2] = data[2]\n",
        "    # 南側の未計測部分を補間する\n",
        "    data[154:] = data[153]\n",
        "    ## 2. 西側の未計測部分を補間する(左右)\n",
        "    # 西側の未計測部分を補間する\n",
        "    data[:, :8] = data[:,8].reshape(-1, 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "# fill_lack_data()関数をmet_dataに対して実行する\n",
        "filled_data = fill_lack_data(met_data)\n",
        "\n",
        "# 最大値の出力\n",
        "print('最大値: ', np.max(met_data))\n",
        "# 最小値の出力\n",
        "print('最小値: ', np.min(met_data))\n",
        "# 平均値の出力\n",
        "print('平均値: ', np.mean(met_data))\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(filled_data, 'gray')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "kg6HUS6_xRIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# npyファイルの画像を表示\n",
        "\n",
        "# ライブラリのインポート\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "source_folder = '/content/drive/MyDrive/2024_AIContest2/Thema2/'\n",
        "\n",
        "# npyを読み込む\n",
        "train_sat_name = os.path.join(source_folder, 'train_sat_re20.npy')\n",
        "train_sat_dataset = np.load(train_sat_name)\n",
        "\n",
        "# 学習用データ、検証用データを抽出する\n",
        "train_sat = train_sat_dataset[:365]\n",
        "val_sat   = train_sat_dataset[365:]\n",
        "\n",
        "# データの形状確認（例: (50, 24, 33, 25)）\n",
        "print(\"データ形状:\", train_sat.shape)\n",
        "\n",
        "flattened = train_sat.reshape(-1, train_sat.shape[2], train_sat.shape[3])\n",
        "images_to_show = flattened[:20]  # 最初の20枚\n",
        "\n",
        "# --- 画像を2×10で並べて表示 ---\n",
        "fig, axes = plt.subplots(2,10, figsize=(15, 15))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(images_to_show[i], cmap='gray', vmin=0, vmax=255)\n",
        "    ax.set_title(f\"#{i}\")\n",
        "    ax.axis('off')  # 枠線・目盛りを非表示\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9LdWYsqhXf0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}